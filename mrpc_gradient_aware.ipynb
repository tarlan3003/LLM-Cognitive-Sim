{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import modules\n",
    "from datasets import load_dataset \n",
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer, BertForSequenceClassification\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "import pickle\n",
    "from torch.utils.data import DataLoader, Sampler, BatchSampler\n",
    "from torch.nn.utils import parameters_to_vector, vector_to_parameters\n",
    "from torch import nn\n",
    "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score, classification_report\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ideas\n",
    "- show that gradients become more similar after gram Schmidt orthogonalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# load model and tokenizer\n",
    "model_orig = AutoModelForSequenceClassification.from_pretrained('bert-base-cased', device_map=\"cuda\")\n",
    "model_diff = AutoModelForSequenceClassification.from_pretrained('bert-base-cased', device_map=\"cuda\")\n",
    "#model_gib = AutoModelForSequenceClassification.from_pretrained('bert-base-cased', device_map=\"cuda\")\n",
    "tokenizer = AutoTokenizer.from_pretrained('bert-base-cased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_dataset(\n",
    "    \"json\",\n",
    "    data_files={\n",
    "      \"train\": \"data/mrpc_gibberish_train_filtered.json\",\n",
    "      \"test\":  \"data/mrpc_gibberish_test_filtered.json\"\n",
    "    },\n",
    "    field=None\n",
    ")\n",
    "\n",
    "# Define an encoding function that preserves the gibberish fields.\n",
    "def encode(examples):\n",
    "    tokenized = tokenizer(\n",
    "        examples[\"sentence1\"],\n",
    "        examples[\"sentence2\"],\n",
    "        truncation=True,\n",
    "        padding=\"max_length\"\n",
    "    )\n",
    "    # Ensure we preserve the original gibberish fields.\n",
    "    tokenized[\"sentence1_gibberish\"] = examples[\"sentence1_gibberish\"]\n",
    "    tokenized[\"sentence2_gibberish\"] = examples[\"sentence2_gibberish\"]\n",
    "    return tokenized\n",
    "\n",
    "# Apply encoding with batched=True (this will preserve keys if you don't remove them).\n",
    "dataset = dataset.map(encode, batched=True)\n",
    "\n",
    "# When mapping the labels, explicitly not remove any columns.\n",
    "dataset = dataset.map(lambda examples: {\"labels\": examples[\"label\"]}, batched=True, remove_columns=[])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset['train'].set_format(\n",
    "    type='torch', \n",
    "    columns=[\n",
    "        'input_ids', 'token_type_ids', 'attention_mask', 'labels',\n",
    "        'sentence1_gibberish', 'sentence2_gibberish'\n",
    "    ]\n",
    ")\n",
    "dataset['test'].set_format(\n",
    "    type='torch', \n",
    "    columns=[\n",
    "        'input_ids', 'token_type_ids', 'attention_mask', 'labels'\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gram_schmidt(v_gib, v_info):\n",
    "    len_orig = torch.pow(torch.dot(v_info, v_info), .5)\n",
    "    len_gib = torch.pow(torch.dot(v_gib, v_gib), .5)\n",
    "    projection = (torch.dot(v_info, v_gib) / torch.dot(v_gib, v_gib)) * v_gib\n",
    "    u2 = v_info - projection\n",
    "    u3 = u2 * (len_orig / len_gib)\n",
    "    return u3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BalancedBatchSampler(Sampler):\n",
    "    def __init__(self, labels, batch_size):\n",
    "        assert batch_size % 2 == 0, \"Batch size must be even for two classes\"\n",
    "        self.labels = labels\n",
    "        self.bs = batch_size\n",
    "        # split indices by class\n",
    "        self.class_indices = {\n",
    "            0: [i for i, lab in enumerate(labels) if lab == 0],\n",
    "            1: [i for i, lab in enumerate(labels) if lab == 1],\n",
    "        }\n",
    "        self.num_batches = min(len(self.class_indices[0]), len(self.class_indices[1])) * 2 // batch_size\n",
    "\n",
    "    def __iter__(self):\n",
    "        # shuffle each class’s indices\n",
    "        for idx_list in self.class_indices.values():\n",
    "            random.shuffle(idx_list)\n",
    "        # yield batch_count batches\n",
    "        for i in range(self.num_batches):\n",
    "            half = self.bs // 2\n",
    "            # take a slice from each class\n",
    "            start = i * half\n",
    "            batch = (\n",
    "                self.class_indices[0][start:start+half] +\n",
    "                self.class_indices[1][start:start+half]\n",
    "            )\n",
    "            random.shuffle(batch)\n",
    "            yield batch\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.num_batches\n",
    "\n",
    "# usage:\n",
    "batch_size = 4 # batch size of 6 is already taking up too much VRAM\n",
    "labels = dataset[\"train\"][\"labels\"]\n",
    "balanced_sampler = BalancedBatchSampler(labels, batch_size=batch_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def my_collate(batch):\n",
    "    return {\n",
    "        'input_ids':      torch.stack([item['input_ids']      for item in batch]), # token IDs like: [ CLS ]  tokens_of_sentence1  [ SEP ]  tokens_of_sentence2  [ SEP ]  padding… \n",
    "        'token_type_ids': torch.stack([item['token_type_ids'] for item in batch]), # whether the token belonged to sentence1 or sentence2\n",
    "        'attention_mask': torch.stack([item['attention_mask'] for item in batch]), # marks which positions are padding vs. real tokens\n",
    "        'labels':         torch.tensor([item['labels']         for item in batch]), # prediction labels (0, 1)\n",
    "        'sentence1_gibberish': [item.get('sentence1_gibberish', []) for item in batch],\n",
    "        'sentence2_gibberish': [item.get('sentence2_gibberish', []) for item in batch],\n",
    "    }\n",
    "\n",
    "# 4) Prepare DataLoaders\n",
    "train_loader = DataLoader(\n",
    "    dataset['train'], batch_sampler=balanced_sampler, collate_fn=my_collate\n",
    ")\n",
    "\"\"\" batch_size = 4\n",
    "train_loader = DataLoader(\n",
    "    dataset['train'], batch_size=batch_size, shuffle=True, collate_fn=my_collate\n",
    ") \"\"\"\n",
    "\n",
    "test_loader = DataLoader(\n",
    "    dataset['test'], batch_size=batch_size, shuffle=False,\n",
    "    collate_fn=lambda b: {\n",
    "        'input_ids': torch.stack([x['input_ids'] for x in b]),\n",
    "        'token_type_ids': torch.stack([x['token_type_ids'] for x in b]),\n",
    "        'attention_mask': torch.stack([x['attention_mask'] for x in b]),\n",
    "        'labels': torch.tensor([x['labels'] for x in b]),\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer_orig = torch.optim.AdamW(model_orig.parameters(), lr=1e-5)\n",
    "optimizer_diff = torch.optim.AdamW(model_diff.parameters(), lr=1e-5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "check for potential errors in the logic: e.g. \n",
    "- is the model updated on the correct gradient vector?\n",
    "- is the Gram-Schmidt process calculated correctly?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' # 6) Training loop on train_loader\\nnum_epochs = 3\\nfor epoch in range(num_epochs):\\n    model_orig.train(); model_diff.train()\\n    for batch_idx, batch in enumerate(tqdm(train_loader, desc=f\"Epoch {epoch+1}\"), start=1):\\n        for k, v in batch.items(): # extracts keys (e.g. \\'input_ids\\') and values (actual tokens) from batch\\n            if torch.is_tensor(v): batch[k] = v.to(device) # moves value to GPU, if it is a token\\n\\n        # unpack\\n        input_ids = batch[\\'input_ids\\']\\n        token_type_ids = batch[\\'token_type_ids\\']\\n        attention_mask = batch[\\'attention_mask\\']\\n        labels = batch[\\'labels\\']\\n        gib1_all = batch[\\'sentence1_gibberish\\']\\n        gib2_all = batch[\\'sentence2_gibberish\\']\\n        num_variants = len(gib1_all[0])\\n\\n        # Pre-tokenize gibberish variants\\n        gib_inputs = []\\n        for j in range(num_variants):\\n            s1 = [g[j] for g in gib1_all]\\n            s2 = [g[j] for g in gib2_all]\\n            enc = tokenizer(s1, s2, truncation=True, padding=\\'max_length\\', return_tensors=\\'pt\\')\\n            gib_inputs.append({k: v.to(device) for k, v in enc.items()})\\n\\n        # A) Original pass\\n        optimizer_orig.zero_grad()\\n        out_orig = model_orig(input_ids=input_ids,\\n                              token_type_ids=token_type_ids,\\n                              attention_mask=attention_mask,\\n                              labels=labels)\\n        loss_orig = out_orig.loss\\n        loss_orig.backward()\\n        optimizer_orig.step()\\n        orig_grads = [p.grad.detach().cpu().clone() for p in model_orig.parameters()]\\n\\n        # B) Gibberish pass\\n        optimizer_orig.zero_grad()\\n        gib_loss = torch.tensor(0.0, device=device)\\n        for enc in gib_inputs:\\n            out = model_orig(**enc, labels=labels)\\n            gib_loss += out.loss\\n        gib_loss /= num_variants\\n        gib_loss.backward()\\n        gib_grads = [p.grad.detach().cpu().clone() for p in model_orig.parameters()]\\n\\n        # C) Compute orthogonal vector\\n        orig_vec = parameters_to_vector(orig_grads)\\n        gib_vec = parameters_to_vector(gib_grads)\\n        v_orth = gram_schmidt(gib_vec, orig_vec)\\n        #v_diff = orig_vec - orig_vec\\n\\n        # D) Update model_diff\\n\\n        optimizer_diff.zero_grad() # clear old grads\\n        # unflatten v_diff → p.grad for each parameter p\\n        pointer = 0\\n        for p in model_diff.parameters():\\n            if not p.requires_grad:\\n                continue\\n            numel = p.numel()\\n            p.grad = v_orth[pointer:pointer+numel].view_as(p).to(device)\\n            pointer += numel\\n        optimizer_diff.step()\\n\\n\\n        if batch_idx % 5 == 0:\\n            print(f\"[Epoch {epoch+1} | Batch {batch_idx}] \"\\n                  f\"orig_loss = {loss_orig.item():.4f}, gib_loss = {gib_loss.item():.4f}\")\\n\\n\\n    print(f\"Epoch {epoch+1} complete\")\\n\\nmodel_orig.save_pretrained(\"trained_model_original\")\\nmodel_diff.save_pretrained(\"trained_model_gradient_diff\") '"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\" # 6) Training loop on train_loader\n",
    "num_epochs = 3\n",
    "for epoch in range(num_epochs):\n",
    "    model_orig.train(); model_diff.train()\n",
    "    for batch_idx, batch in enumerate(tqdm(train_loader, desc=f\"Epoch {epoch+1}\"), start=1):\n",
    "        for k, v in batch.items(): # extracts keys (e.g. 'input_ids') and values (actual tokens) from batch\n",
    "            if torch.is_tensor(v): batch[k] = v.to(device) # moves value to GPU, if it is a token\n",
    "\n",
    "        # unpack\n",
    "        input_ids = batch['input_ids']\n",
    "        token_type_ids = batch['token_type_ids']\n",
    "        attention_mask = batch['attention_mask']\n",
    "        labels = batch['labels']\n",
    "        gib1_all = batch['sentence1_gibberish']\n",
    "        gib2_all = batch['sentence2_gibberish']\n",
    "        num_variants = len(gib1_all[0])\n",
    "\n",
    "        # Pre-tokenize gibberish variants\n",
    "        gib_inputs = []\n",
    "        for j in range(num_variants):\n",
    "            s1 = [g[j] for g in gib1_all]\n",
    "            s2 = [g[j] for g in gib2_all]\n",
    "            enc = tokenizer(s1, s2, truncation=True, padding='max_length', return_tensors='pt')\n",
    "            gib_inputs.append({k: v.to(device) for k, v in enc.items()})\n",
    "\n",
    "        # A) Original pass\n",
    "        optimizer_orig.zero_grad()\n",
    "        out_orig = model_orig(input_ids=input_ids,\n",
    "                              token_type_ids=token_type_ids,\n",
    "                              attention_mask=attention_mask,\n",
    "                              labels=labels)\n",
    "        loss_orig = out_orig.loss\n",
    "        loss_orig.backward()\n",
    "        optimizer_orig.step()\n",
    "        orig_grads = [p.grad.detach().cpu().clone() for p in model_orig.parameters()]\n",
    "\n",
    "        # B) Gibberish pass\n",
    "        optimizer_orig.zero_grad()\n",
    "        gib_loss = torch.tensor(0.0, device=device)\n",
    "        for enc in gib_inputs:\n",
    "            out = model_orig(**enc, labels=labels)\n",
    "            gib_loss += out.loss\n",
    "        gib_loss /= num_variants\n",
    "        gib_loss.backward()\n",
    "        gib_grads = [p.grad.detach().cpu().clone() for p in model_orig.parameters()]\n",
    "\n",
    "        # C) Compute orthogonal vector\n",
    "        orig_vec = parameters_to_vector(orig_grads)\n",
    "        gib_vec = parameters_to_vector(gib_grads)\n",
    "        v_orth = gram_schmidt(gib_vec, orig_vec)\n",
    "        #v_diff = orig_vec - orig_vec\n",
    "\n",
    "        # D) Update model_diff\n",
    "\n",
    "        optimizer_diff.zero_grad() # clear old grads\n",
    "        # unflatten v_diff → p.grad for each parameter p\n",
    "        pointer = 0\n",
    "        for p in model_diff.parameters():\n",
    "            if not p.requires_grad:\n",
    "                continue\n",
    "            numel = p.numel()\n",
    "            p.grad = v_orth[pointer:pointer+numel].view_as(p).to(device)\n",
    "            pointer += numel\n",
    "        optimizer_diff.step()\n",
    "\n",
    "\n",
    "        if batch_idx % 5 == 0:\n",
    "            print(f\"[Epoch {epoch+1} | Batch {batch_idx}] \"\n",
    "                  f\"orig_loss = {loss_orig.item():.4f}, gib_loss = {gib_loss.item():.4f}\")\n",
    "\n",
    "\n",
    "    print(f\"Epoch {epoch+1} complete\")\n",
    "\n",
    "model_orig.save_pretrained(\"trained_model_original\")\n",
    "model_diff.save_pretrained(\"trained_model_gradient_diff\") \"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "note: model seems to be 100% sure, that every pair is not the same"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1:   5%|▍         | 5/106 [00:27<08:58,  5.33s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 1 | Batch 5] orig_loss = 0.6323, gib_loss = 0.7383, labels: tensor([0, 1, 0, 1], device='cuda:0')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1:   9%|▉         | 10/106 [00:51<07:38,  4.78s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 1 | Batch 10] orig_loss = 0.8053, gib_loss = 0.7138, labels: tensor([0, 0, 1, 1], device='cuda:0')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1:  14%|█▍        | 15/106 [01:15<07:10,  4.73s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 1 | Batch 15] orig_loss = 0.7125, gib_loss = 0.7186, labels: tensor([1, 0, 1, 0], device='cuda:0')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1:  19%|█▉        | 20/106 [01:41<07:23,  5.16s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 1 | Batch 20] orig_loss = 0.6660, gib_loss = 0.7023, labels: tensor([0, 0, 1, 1], device='cuda:0')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1:  24%|██▎       | 25/106 [02:06<06:32,  4.84s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 1 | Batch 25] orig_loss = 0.7219, gib_loss = 0.6866, labels: tensor([0, 1, 0, 1], device='cuda:0')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1:  28%|██▊       | 30/106 [02:31<06:21,  5.02s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 1 | Batch 30] orig_loss = 0.6593, gib_loss = 0.7290, labels: tensor([0, 0, 1, 1], device='cuda:0')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1:  33%|███▎      | 35/106 [02:55<05:53,  4.98s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 1 | Batch 35] orig_loss = 0.7613, gib_loss = 0.7208, labels: tensor([0, 1, 1, 0], device='cuda:0')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1:  38%|███▊      | 40/106 [03:20<05:26,  4.95s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 1 | Batch 40] orig_loss = 0.7249, gib_loss = 0.7091, labels: tensor([0, 1, 1, 0], device='cuda:0')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1:  42%|████▏     | 45/106 [03:44<04:51,  4.77s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 1 | Batch 45] orig_loss = 0.7078, gib_loss = 0.7144, labels: tensor([1, 0, 0, 1], device='cuda:0')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1:  47%|████▋     | 50/106 [04:11<04:42,  5.04s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 1 | Batch 50] orig_loss = 0.6535, gib_loss = 0.7348, labels: tensor([0, 1, 0, 1], device='cuda:0')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1:  52%|█████▏    | 55/106 [04:36<04:15,  5.01s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 1 | Batch 55] orig_loss = 0.7362, gib_loss = 0.6847, labels: tensor([0, 1, 0, 1], device='cuda:0')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1:  57%|█████▋    | 60/106 [05:01<03:53,  5.08s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 1 | Batch 60] orig_loss = 0.6557, gib_loss = 0.6931, labels: tensor([1, 1, 0, 0], device='cuda:0')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1:  61%|██████▏   | 65/106 [05:27<03:38,  5.33s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 1 | Batch 65] orig_loss = 0.6149, gib_loss = 0.7477, labels: tensor([0, 1, 1, 0], device='cuda:0')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1:  66%|██████▌   | 70/106 [05:52<03:01,  5.05s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 1 | Batch 70] orig_loss = 0.6049, gib_loss = 0.7024, labels: tensor([1, 0, 1, 0], device='cuda:0')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1:  71%|███████   | 75/106 [06:17<02:34,  5.00s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 1 | Batch 75] orig_loss = 0.6635, gib_loss = 0.7244, labels: tensor([1, 0, 0, 1], device='cuda:0')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1:  75%|███████▌  | 80/106 [06:41<02:04,  4.80s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 1 | Batch 80] orig_loss = 0.7032, gib_loss = 0.7442, labels: tensor([1, 0, 1, 0], device='cuda:0')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1:  80%|████████  | 85/106 [07:05<01:39,  4.74s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 1 | Batch 85] orig_loss = 0.5982, gib_loss = 0.6998, labels: tensor([0, 0, 1, 1], device='cuda:0')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1:  85%|████████▍ | 90/106 [07:29<01:16,  4.79s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 1 | Batch 90] orig_loss = 0.6702, gib_loss = 0.6349, labels: tensor([1, 0, 0, 1], device='cuda:0')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1:  90%|████████▉ | 95/106 [07:53<00:53,  4.82s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 1 | Batch 95] orig_loss = 0.5417, gib_loss = 0.6860, labels: tensor([0, 1, 0, 1], device='cuda:0')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1:  94%|█████████▍| 100/106 [08:17<00:28,  4.81s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 1 | Batch 100] orig_loss = 0.6549, gib_loss = 0.7727, labels: tensor([0, 0, 1, 1], device='cuda:0')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1:  99%|█████████▉| 105/106 [08:41<00:04,  4.80s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 1 | Batch 105] orig_loss = 0.6966, gib_loss = 0.7280, labels: tensor([0, 1, 1, 0], device='cuda:0')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1: 100%|██████████| 106/106 [08:46<00:00,  4.96s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 complete\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2:   5%|▍         | 5/106 [00:23<08:04,  4.80s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 2 | Batch 5] orig_loss = 0.7060, gib_loss = 0.6820, labels: tensor([0, 1, 0, 1], device='cuda:0')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2:   9%|▉         | 10/106 [00:47<07:31,  4.70s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 2 | Batch 10] orig_loss = 0.7240, gib_loss = 0.8147, labels: tensor([1, 0, 1, 0], device='cuda:0')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2:  14%|█▍        | 15/106 [01:10<07:09,  4.72s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 2 | Batch 15] orig_loss = 0.6826, gib_loss = 0.7620, labels: tensor([1, 0, 1, 0], device='cuda:0')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2:  19%|█▉        | 20/106 [01:34<06:56,  4.84s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 2 | Batch 20] orig_loss = 0.6282, gib_loss = 0.7129, labels: tensor([1, 1, 0, 0], device='cuda:0')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2:  24%|██▎       | 25/106 [01:59<06:28,  4.79s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 2 | Batch 25] orig_loss = 0.5563, gib_loss = 0.6589, labels: tensor([1, 0, 0, 1], device='cuda:0')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2:  28%|██▊       | 30/106 [02:22<06:04,  4.80s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 2 | Batch 30] orig_loss = 0.6847, gib_loss = 0.8357, labels: tensor([1, 0, 0, 1], device='cuda:0')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2:  33%|███▎      | 35/106 [02:48<06:00,  5.08s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 2 | Batch 35] orig_loss = 0.6106, gib_loss = 0.7924, labels: tensor([1, 1, 0, 0], device='cuda:0')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2:  38%|███▊      | 40/106 [03:13<05:26,  4.95s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 2 | Batch 40] orig_loss = 0.6151, gib_loss = 0.7262, labels: tensor([0, 1, 1, 0], device='cuda:0')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2:  42%|████▏     | 45/106 [03:39<05:30,  5.42s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 2 | Batch 45] orig_loss = 0.5511, gib_loss = 0.6906, labels: tensor([1, 0, 1, 0], device='cuda:0')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2:  47%|████▋     | 50/106 [04:07<04:58,  5.33s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 2 | Batch 50] orig_loss = 0.3835, gib_loss = 0.8158, labels: tensor([0, 1, 0, 1], device='cuda:0')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2:  52%|█████▏    | 55/106 [04:33<04:30,  5.31s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 2 | Batch 55] orig_loss = 0.4003, gib_loss = 0.9217, labels: tensor([1, 1, 0, 0], device='cuda:0')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2:  57%|█████▋    | 60/106 [05:04<04:28,  5.84s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 2 | Batch 60] orig_loss = 0.6665, gib_loss = 0.8620, labels: tensor([0, 1, 1, 0], device='cuda:0')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2:  61%|██████▏   | 65/106 [05:29<03:33,  5.20s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 2 | Batch 65] orig_loss = 0.9601, gib_loss = 1.0249, labels: tensor([0, 1, 1, 0], device='cuda:0')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2:  66%|██████▌   | 70/106 [05:55<03:06,  5.19s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 2 | Batch 70] orig_loss = 0.5189, gib_loss = 0.8480, labels: tensor([0, 1, 0, 1], device='cuda:0')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2:  71%|███████   | 75/106 [06:20<02:31,  4.90s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 2 | Batch 75] orig_loss = 0.9064, gib_loss = 0.8030, labels: tensor([1, 1, 0, 0], device='cuda:0')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2:  75%|███████▌  | 80/106 [06:45<02:07,  4.91s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 2 | Batch 80] orig_loss = 0.5509, gib_loss = 0.7753, labels: tensor([0, 1, 0, 1], device='cuda:0')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2:  80%|████████  | 85/106 [07:11<01:45,  5.03s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 2 | Batch 85] orig_loss = 1.1013, gib_loss = 0.8141, labels: tensor([0, 0, 1, 1], device='cuda:0')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2:  85%|████████▍ | 90/106 [07:35<01:17,  4.86s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 2 | Batch 90] orig_loss = 0.4250, gib_loss = 0.7508, labels: tensor([0, 1, 1, 0], device='cuda:0')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2:  90%|████████▉ | 95/106 [07:59<00:52,  4.77s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 2 | Batch 95] orig_loss = 0.6422, gib_loss = 0.6310, labels: tensor([1, 1, 0, 0], device='cuda:0')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2:  94%|█████████▍| 100/106 [08:23<00:28,  4.83s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 2 | Batch 100] orig_loss = 0.5634, gib_loss = 0.7189, labels: tensor([1, 0, 0, 1], device='cuda:0')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2:  99%|█████████▉| 105/106 [08:47<00:04,  4.82s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 2 | Batch 105] orig_loss = 0.6642, gib_loss = 0.8595, labels: tensor([1, 1, 0, 0], device='cuda:0')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2: 100%|██████████| 106/106 [08:52<00:00,  5.02s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 complete\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3:   5%|▍         | 5/106 [00:26<09:07,  5.43s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 3 | Batch 5] orig_loss = 0.4269, gib_loss = 0.6832, labels: tensor([0, 1, 0, 1], device='cuda:0')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3:   9%|▉         | 10/106 [00:50<07:44,  4.84s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 3 | Batch 10] orig_loss = 0.4033, gib_loss = 0.9540, labels: tensor([1, 1, 0, 0], device='cuda:0')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3:  14%|█▍        | 15/106 [01:14<07:21,  4.85s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 3 | Batch 15] orig_loss = 0.3676, gib_loss = 0.7909, labels: tensor([0, 1, 1, 0], device='cuda:0')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3:  19%|█▉        | 20/106 [01:38<06:49,  4.77s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 3 | Batch 20] orig_loss = 0.3572, gib_loss = 0.6016, labels: tensor([0, 0, 1, 1], device='cuda:0')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3:  24%|██▎       | 25/106 [02:01<06:21,  4.71s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 3 | Batch 25] orig_loss = 0.2336, gib_loss = 0.8255, labels: tensor([0, 0, 1, 1], device='cuda:0')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3:  28%|██▊       | 30/106 [02:25<06:03,  4.79s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 3 | Batch 30] orig_loss = 0.2843, gib_loss = 0.9713, labels: tensor([0, 1, 0, 1], device='cuda:0')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3:  33%|███▎      | 35/106 [02:49<05:36,  4.74s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 3 | Batch 35] orig_loss = 0.4597, gib_loss = 1.0472, labels: tensor([1, 0, 0, 1], device='cuda:0')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3:  38%|███▊      | 40/106 [03:13<05:19,  4.84s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 3 | Batch 40] orig_loss = 0.7253, gib_loss = 1.0565, labels: tensor([0, 1, 0, 1], device='cuda:0')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3:  42%|████▏     | 45/106 [03:39<05:03,  4.97s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 3 | Batch 45] orig_loss = 0.9045, gib_loss = 0.9498, labels: tensor([0, 1, 0, 1], device='cuda:0')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3:  47%|████▋     | 50/106 [04:04<04:35,  4.93s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 3 | Batch 50] orig_loss = 0.1891, gib_loss = 0.7889, labels: tensor([1, 1, 0, 0], device='cuda:0')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3:  52%|█████▏    | 55/106 [04:28<04:05,  4.82s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 3 | Batch 55] orig_loss = 0.4914, gib_loss = 0.8314, labels: tensor([0, 1, 1, 0], device='cuda:0')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3:  57%|█████▋    | 60/106 [04:53<03:46,  4.92s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 3 | Batch 60] orig_loss = 0.3435, gib_loss = 1.0871, labels: tensor([0, 1, 1, 0], device='cuda:0')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3:  61%|██████▏   | 65/106 [05:17<03:19,  4.86s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 3 | Batch 65] orig_loss = 0.4184, gib_loss = 1.0299, labels: tensor([0, 1, 0, 1], device='cuda:0')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3:  66%|██████▌   | 70/106 [05:42<02:55,  4.89s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 3 | Batch 70] orig_loss = 0.8542, gib_loss = 1.3246, labels: tensor([1, 1, 0, 0], device='cuda:0')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3:  71%|███████   | 75/106 [06:08<02:39,  5.16s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 3 | Batch 75] orig_loss = 0.5793, gib_loss = 1.2910, labels: tensor([1, 0, 1, 0], device='cuda:0')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3:  75%|███████▌  | 80/106 [06:34<02:22,  5.48s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 3 | Batch 80] orig_loss = 0.5228, gib_loss = 1.0546, labels: tensor([0, 1, 1, 0], device='cuda:0')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3:  80%|████████  | 85/106 [07:00<01:47,  5.11s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 3 | Batch 85] orig_loss = 0.5033, gib_loss = 1.1962, labels: tensor([1, 0, 1, 0], device='cuda:0')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3:  85%|████████▍ | 90/106 [07:26<01:25,  5.33s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 3 | Batch 90] orig_loss = 0.4181, gib_loss = 1.1734, labels: tensor([0, 1, 0, 1], device='cuda:0')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3:  90%|████████▉ | 95/106 [07:52<00:56,  5.12s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 3 | Batch 95] orig_loss = 0.6918, gib_loss = 1.2003, labels: tensor([0, 1, 0, 1], device='cuda:0')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3:  94%|█████████▍| 100/106 [08:17<00:29,  4.95s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 3 | Batch 100] orig_loss = 0.3919, gib_loss = 0.6420, labels: tensor([0, 1, 1, 0], device='cuda:0')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3:  99%|█████████▉| 105/106 [08:43<00:05,  5.03s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 3 | Batch 105] orig_loss = 0.3217, gib_loss = 0.5815, labels: tensor([0, 1, 1, 0], device='cuda:0')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3: 100%|██████████| 106/106 [08:48<00:00,  4.99s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 complete\n"
     ]
    }
   ],
   "source": [
    "# 6) Training loop on train_loader\n",
    "num_epochs = 3\n",
    "for epoch in range(num_epochs):\n",
    "    model_orig.train(); model_diff.train()\n",
    "    for batch_idx, batch in enumerate(tqdm(train_loader, desc=f\"Epoch {epoch+1}\"), start=1):\n",
    "        for k, v in batch.items(): # extracts keys (e.g. 'input_ids') and values (actual tokens) from batch\n",
    "            if torch.is_tensor(v): batch[k] = v.to(device) # moves value to GPU, if it is a token\n",
    "\n",
    "        # unpack\n",
    "        input_ids = batch['input_ids']\n",
    "        token_type_ids = batch['token_type_ids']\n",
    "        attention_mask = batch['attention_mask']\n",
    "        labels = batch['labels']\n",
    "        gib1_all = batch['sentence1_gibberish']\n",
    "        gib2_all = batch['sentence2_gibberish']\n",
    "        num_variants = len(gib1_all[0])\n",
    "\n",
    "        # Pre-tokenize gibberish variants\n",
    "        gib_inputs = []\n",
    "        for j in range(num_variants):\n",
    "            s1 = [g[j] for g in gib1_all]\n",
    "            s2 = [g[j] for g in gib2_all]\n",
    "            enc = tokenizer(s1, s2, truncation=True, padding='max_length', return_tensors='pt')\n",
    "            gib_inputs.append({k: v.to(device) for k, v in enc.items()})\n",
    "\n",
    "        # A) Original pass\n",
    "        optimizer_orig.zero_grad()\n",
    "        out_orig = model_orig(input_ids=input_ids,\n",
    "                              token_type_ids=token_type_ids,\n",
    "                              attention_mask=attention_mask,\n",
    "                              labels=labels)\n",
    "        loss_orig = out_orig.loss\n",
    "        loss_orig.backward()\n",
    "        optimizer_orig.step()\n",
    "        #orig_grads = [p.grad.detach().cpu().clone() for p in model_orig.parameters()]\n",
    "\n",
    "\n",
    "        # B) Gibberish pass\n",
    "        # B.1 pass for original data\n",
    "        optimizer_diff.zero_grad()\n",
    "        out_orig_model_diff = model_diff(input_ids=input_ids,\n",
    "                                            token_type_ids=token_type_ids,\n",
    "                                            attention_mask=attention_mask,\n",
    "                                            labels=labels)\n",
    "        loss_orig_model_diff = out_orig_model_diff.loss\n",
    "        loss_orig_model_diff.backward()\n",
    "        orig_grads_model_diff = [p.grad.detach().cpu().clone() for p in model_diff.parameters()]\n",
    "\n",
    "        # B.2 pass for gibberish data\n",
    "        optimizer_diff.zero_grad()\n",
    "        gib_loss = torch.tensor(0.0, device=device)\n",
    "        for enc in gib_inputs:\n",
    "            out = model_diff(**enc, labels=labels)\n",
    "            gib_loss += out.loss\n",
    "        gib_loss /= num_variants\n",
    "        gib_loss.backward()\n",
    "        gib_grads = [p.grad.detach().cpu().clone() for p in model_diff.parameters()]\n",
    "\n",
    "        # C) Compute orthogonal vector\n",
    "        orig_vec_model_diff = parameters_to_vector(orig_grads_model_diff)\n",
    "        gib_vec = parameters_to_vector(gib_grads)\n",
    "        v_orth = gram_schmidt(gib_vec, orig_vec_model_diff)\n",
    "        #v_diff = orig_vec - orig_vec\n",
    "\n",
    "        # D) Update model_diff\n",
    "        optimizer_diff.zero_grad() # clear old grads\n",
    "        # unflatten v_orth → p.grad for each parameter p\n",
    "        pointer = 0\n",
    "        for p in model_diff.parameters():\n",
    "            if not p.requires_grad:\n",
    "                continue\n",
    "            numel = p.numel()\n",
    "            p.grad = v_orth[pointer:pointer+numel].view_as(p).to(device)\n",
    "            pointer += numel\n",
    "        optimizer_diff.step()\n",
    "\n",
    "\n",
    "        if batch_idx % 5 == 0:\n",
    "            print(f\"[Epoch {epoch+1} | Batch {batch_idx}] \"\n",
    "                  f\"orig_loss = {loss_orig.item():.4f}, gib_loss = {gib_loss.item():.4f}, labels: {labels[:]}\")\n",
    "\n",
    "\n",
    "    print(f\"Epoch {epoch+1} complete\")\n",
    "\n",
    "model_orig.save_pretrained(\"trained_model_original_2\")\n",
    "model_diff.save_pretrained(\"trained_model_gradient_orth_2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Model Metrics:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.8000    0.3448    0.4819        58\n",
      "           1     0.7286    0.9533    0.8259       107\n",
      "\n",
      "    accuracy                         0.7394       165\n",
      "   macro avg     0.7643    0.6490    0.6539       165\n",
      "weighted avg     0.7537    0.7394    0.7050       165\n",
      "\n",
      "Orthogonal trained Model Metrics:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.3515    1.0000    0.5202        58\n",
      "           1     0.0000    0.0000    0.0000       107\n",
      "\n",
      "    accuracy                         0.3515       165\n",
      "   macro avg     0.1758    0.5000    0.2601       165\n",
      "weighted avg     0.1236    0.3515    0.1829       165\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tschuetz/.conda/envs/tilo_env/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/home/tschuetz/.conda/envs/tilo_env/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/home/tschuetz/.conda/envs/tilo_env/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    }
   ],
   "source": [
    "# 7) Evaluation on test_loader\n",
    "def evaluate(model, loader):\n",
    "    model.eval()\n",
    "    all_preds, all_labels = [], []\n",
    "    with torch.no_grad():\n",
    "        for batch in loader:\n",
    "            batch = {k: v.to(device) for k, v in batch.items()}\n",
    "            logits = model(\n",
    "                input_ids=batch['input_ids'],\n",
    "                token_type_ids=batch['token_type_ids'],\n",
    "                attention_mask=batch['attention_mask']\n",
    "            ).logits\n",
    "            preds = torch.argmax(logits, dim=-1)\n",
    "            all_preds.extend(preds.cpu().tolist())\n",
    "            all_labels.extend(batch['labels'].cpu().tolist())\n",
    "    return all_labels, all_preds\n",
    "\n",
    "model_orig = BertForSequenceClassification.from_pretrained(\"./trained_model_original\")\n",
    "model_orig.to(device)\n",
    "\n",
    "model_diff = BertForSequenceClassification.from_pretrained(\"./trained_model_gradient_diff\")\n",
    "model_diff.to(device)\n",
    "\n",
    "# Compute metrics for each model\n",
    "labels_o, preds_o = evaluate(model_orig, test_loader)\n",
    "labels_d, preds_d = evaluate(model_diff, test_loader)\n",
    "\n",
    "print(\"Original Model Metrics:\")\n",
    "print(classification_report(labels_o, preds_o, digits=4))\n",
    "\n",
    "print(\"Orthogonal trained Model Metrics:\")\n",
    "print(classification_report(labels_d, preds_d, digits=4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "thoughts: the models learns at least in some direction, as it always predicts the minority class. Usually, it is not incentivized to do that"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1:   5%|▍         | 5/106 [00:24<08:09,  4.84s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 1 | Batch 5] orig_loss = 0.6867, gib_loss = 0.8929\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1:   9%|▉         | 10/106 [00:53<08:34,  5.35s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 1 | Batch 10] orig_loss = 0.0779, gib_loss = 0.8759\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1:  14%|█▍        | 15/106 [01:17<07:26,  4.90s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 1 | Batch 15] orig_loss = 0.0631, gib_loss = 0.8751\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1:  19%|█▉        | 20/106 [01:43<07:21,  5.14s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 1 | Batch 20] orig_loss = 0.0870, gib_loss = 0.8773\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1:  24%|██▎       | 25/106 [02:08<06:50,  5.07s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 1 | Batch 25] orig_loss = 0.0744, gib_loss = 0.8650\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1:  28%|██▊       | 30/106 [02:33<06:13,  4.92s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 1 | Batch 30] orig_loss = 0.1191, gib_loss = 0.8751\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1:  33%|███▎      | 35/106 [02:57<05:43,  4.84s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 1 | Batch 35] orig_loss = 0.2665, gib_loss = 0.8509\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1:  38%|███▊      | 40/106 [03:22<05:39,  5.15s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 1 | Batch 40] orig_loss = 0.1212, gib_loss = 0.8924\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1:  42%|████▏     | 45/106 [03:47<05:05,  5.00s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 1 | Batch 45] orig_loss = 0.0843, gib_loss = 0.8786\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1:  47%|████▋     | 50/106 [04:12<04:44,  5.08s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 1 | Batch 50] orig_loss = 0.0673, gib_loss = 0.8770\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1:  52%|█████▏    | 55/106 [04:37<04:06,  4.83s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 1 | Batch 55] orig_loss = 0.7216, gib_loss = 0.8834\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1:  57%|█████▋    | 60/106 [05:02<03:43,  4.86s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 1 | Batch 60] orig_loss = 0.0623, gib_loss = 0.9109\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1:  61%|██████▏   | 65/106 [05:27<03:18,  4.84s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 1 | Batch 65] orig_loss = 0.0694, gib_loss = 0.8846\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1:  66%|██████▌   | 70/106 [05:51<02:56,  4.91s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 1 | Batch 70] orig_loss = 0.1864, gib_loss = 0.8662\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1:  71%|███████   | 75/106 [06:15<02:30,  4.85s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 1 | Batch 75] orig_loss = 0.1820, gib_loss = 0.8645\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1:  75%|███████▌  | 80/106 [06:40<02:09,  4.99s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 1 | Batch 80] orig_loss = 0.1021, gib_loss = 0.8968\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1:  80%|████████  | 85/106 [07:08<01:59,  5.70s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 1 | Batch 85] orig_loss = 0.6820, gib_loss = 0.8737\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1:  85%|████████▍ | 90/106 [07:34<01:21,  5.12s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 1 | Batch 90] orig_loss = 0.0648, gib_loss = 0.8973\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1:  90%|████████▉ | 95/106 [07:58<00:53,  4.88s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 1 | Batch 95] orig_loss = 0.7379, gib_loss = 0.8934\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1:  94%|█████████▍| 100/106 [08:22<00:28,  4.80s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 1 | Batch 100] orig_loss = 0.0666, gib_loss = 0.8575\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1:  99%|█████████▉| 105/106 [08:45<00:04,  4.72s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 1 | Batch 105] orig_loss = 0.1566, gib_loss = 0.8902\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1: 100%|██████████| 106/106 [08:50<00:00,  5.01s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 complete\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2:   5%|▍         | 5/106 [00:24<08:13,  4.89s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 2 | Batch 5] orig_loss = 0.4170, gib_loss = 0.9030\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2:   9%|▉         | 10/106 [00:49<08:00,  5.01s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 2 | Batch 10] orig_loss = 0.2418, gib_loss = 0.8673\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2:  14%|█▍        | 15/106 [01:13<07:29,  4.93s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 2 | Batch 15] orig_loss = 0.5622, gib_loss = 0.8813\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2:  19%|█▉        | 20/106 [01:38<06:59,  4.88s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 2 | Batch 20] orig_loss = 0.0769, gib_loss = 0.8690\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2:  24%|██▎       | 25/106 [02:05<07:12,  5.34s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 2 | Batch 25] orig_loss = 0.1499, gib_loss = 0.8797\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2:  28%|██▊       | 30/106 [02:30<06:34,  5.19s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 2 | Batch 30] orig_loss = 0.0679, gib_loss = 0.8691\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2:  33%|███▎      | 35/106 [02:55<05:50,  4.94s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 2 | Batch 35] orig_loss = 0.1923, gib_loss = 0.8965\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2:  38%|███▊      | 40/106 [03:21<05:42,  5.19s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 2 | Batch 40] orig_loss = 0.6538, gib_loss = 0.8874\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2:  42%|████▏     | 45/106 [03:45<04:54,  4.83s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 2 | Batch 45] orig_loss = 0.0799, gib_loss = 0.8898\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2:  47%|████▋     | 50/106 [04:08<04:28,  4.79s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 2 | Batch 50] orig_loss = 0.0854, gib_loss = 0.8957\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2:  52%|█████▏    | 55/106 [04:34<04:21,  5.13s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 2 | Batch 55] orig_loss = 0.5794, gib_loss = 0.8919\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2:  57%|█████▋    | 60/106 [04:58<03:40,  4.79s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 2 | Batch 60] orig_loss = 0.4497, gib_loss = 0.8940\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2:  61%|██████▏   | 65/106 [05:23<03:24,  5.00s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 2 | Batch 65] orig_loss = 0.0822, gib_loss = 0.8784\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2:  66%|██████▌   | 70/106 [05:48<03:03,  5.10s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 2 | Batch 70] orig_loss = 0.1147, gib_loss = 0.8573\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2:  71%|███████   | 75/106 [06:11<02:27,  4.77s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 2 | Batch 75] orig_loss = 0.6113, gib_loss = 0.8767\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2:  75%|███████▌  | 80/106 [06:38<02:19,  5.35s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 2 | Batch 80] orig_loss = 0.6381, gib_loss = 0.8983\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2:  80%|████████  | 85/106 [07:04<01:47,  5.11s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 2 | Batch 85] orig_loss = 0.0995, gib_loss = 0.8694\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2:  85%|████████▍ | 90/106 [07:30<01:23,  5.24s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 2 | Batch 90] orig_loss = 0.1042, gib_loss = 0.8833\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2:  90%|████████▉ | 95/106 [07:54<00:54,  4.97s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 2 | Batch 95] orig_loss = 0.0586, gib_loss = 0.8678\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2:  94%|█████████▍| 100/106 [08:19<00:29,  4.89s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 2 | Batch 100] orig_loss = 0.5897, gib_loss = 0.9062\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2:  99%|█████████▉| 105/106 [08:44<00:04,  4.83s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 2 | Batch 105] orig_loss = 0.3362, gib_loss = 0.8659\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2: 100%|██████████| 106/106 [08:49<00:00,  5.00s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 complete\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3:   5%|▍         | 5/106 [00:24<08:10,  4.85s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 3 | Batch 5] orig_loss = 0.6408, gib_loss = 0.8656\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3:   9%|▉         | 10/106 [00:50<08:12,  5.13s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 3 | Batch 10] orig_loss = 0.1175, gib_loss = 0.8735\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3:  14%|█▍        | 15/106 [01:15<07:36,  5.01s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 3 | Batch 15] orig_loss = 0.0756, gib_loss = 0.8722\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3:  19%|█▉        | 20/106 [01:42<07:43,  5.39s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 3 | Batch 20] orig_loss = 0.2582, gib_loss = 0.8890\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3:  24%|██▎       | 25/106 [02:08<06:51,  5.08s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 3 | Batch 25] orig_loss = 0.5383, gib_loss = 0.8815\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3:  28%|██▊       | 30/106 [02:34<06:21,  5.02s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 3 | Batch 30] orig_loss = 0.1173, gib_loss = 0.8688\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3:  33%|███▎      | 35/106 [02:58<05:37,  4.75s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 3 | Batch 35] orig_loss = 0.0693, gib_loss = 0.8789\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3:  38%|███▊      | 40/106 [03:24<05:41,  5.18s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 3 | Batch 40] orig_loss = 0.0641, gib_loss = 0.8350\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3:  42%|████▏     | 45/106 [03:50<05:08,  5.05s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 3 | Batch 45] orig_loss = 0.0654, gib_loss = 0.8769\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3:  47%|████▋     | 50/106 [04:20<05:36,  6.02s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 3 | Batch 50] orig_loss = 0.0676, gib_loss = 0.8545\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3:  52%|█████▏    | 55/106 [04:46<04:34,  5.39s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 3 | Batch 55] orig_loss = 0.1048, gib_loss = 0.9162\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3:  57%|█████▋    | 60/106 [05:10<03:47,  4.95s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 3 | Batch 60] orig_loss = 0.5967, gib_loss = 0.8841\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3:  61%|██████▏   | 65/106 [05:36<03:25,  5.02s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 3 | Batch 65] orig_loss = 0.6134, gib_loss = 0.8732\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3:  66%|██████▌   | 70/106 [06:02<03:10,  5.28s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 3 | Batch 70] orig_loss = 0.0778, gib_loss = 0.9023\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3:  71%|███████   | 75/106 [06:27<02:32,  4.93s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 3 | Batch 75] orig_loss = 0.0892, gib_loss = 0.8672\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3:  75%|███████▌  | 80/106 [06:51<02:07,  4.89s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 3 | Batch 80] orig_loss = 0.0913, gib_loss = 0.8776\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3:  80%|████████  | 85/106 [07:20<01:57,  5.58s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 3 | Batch 85] orig_loss = 0.0635, gib_loss = 0.8620\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3:  85%|████████▍ | 90/106 [07:45<01:21,  5.06s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 3 | Batch 90] orig_loss = 0.1243, gib_loss = 0.9107\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3:  90%|████████▉ | 95/106 [08:10<00:55,  5.02s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 3 | Batch 95] orig_loss = 0.6639, gib_loss = 0.8524\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3:  94%|█████████▍| 100/106 [08:37<00:33,  5.51s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 3 | Batch 100] orig_loss = 0.0612, gib_loss = 0.8719\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3:  99%|█████████▉| 105/106 [09:04<00:05,  5.26s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 3 | Batch 105] orig_loss = 0.6191, gib_loss = 0.9001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3: 100%|██████████| 106/106 [09:09<00:00,  5.18s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 complete\n"
     ]
    }
   ],
   "source": [
    "# 6) Training loop on train_loader\n",
    "num_epochs = 3\n",
    "for epoch in range(num_epochs):\n",
    "    model_orig.train(); model_diff.train()\n",
    "    for batch_idx, batch in enumerate(tqdm(train_loader, desc=f\"Epoch {epoch+1}\"), start=1):\n",
    "        for k, v in batch.items(): # extracts keys (e.g. 'input_ids') and values (actual tokens) from batch\n",
    "            if torch.is_tensor(v): batch[k] = v.to(device) # moves value to GPU, if it is a token\n",
    "\n",
    "        # unpack\n",
    "        input_ids = batch['input_ids']\n",
    "        token_type_ids = batch['token_type_ids']\n",
    "        attention_mask = batch['attention_mask']\n",
    "        labels = batch['labels']\n",
    "        gib1_all = batch['sentence1_gibberish']\n",
    "        gib2_all = batch['sentence2_gibberish']\n",
    "        num_variants = len(gib1_all[0])\n",
    "\n",
    "        # Pre-tokenize gibberish variants\n",
    "        gib_inputs = []\n",
    "        for j in range(num_variants):\n",
    "            s1 = [g[j] for g in gib1_all]\n",
    "            s2 = [g[j] for g in gib2_all]\n",
    "            enc = tokenizer(s1, s2, truncation=True, padding='max_length', return_tensors='pt')\n",
    "            gib_inputs.append({k: v.to(device) for k, v in enc.items()})\n",
    "\n",
    "        # A) Original pass\n",
    "        optimizer_orig.zero_grad()\n",
    "        out_orig = model_orig(input_ids=input_ids,\n",
    "                              token_type_ids=token_type_ids,\n",
    "                              attention_mask=attention_mask,\n",
    "                              labels=labels)\n",
    "        loss_orig = out_orig.loss\n",
    "        loss_orig.backward()\n",
    "        optimizer_orig.step()\n",
    "        #orig_grads = [p.grad.detach().cpu().clone() for p in model_orig.parameters()]\n",
    "\n",
    "\n",
    "        # B) Gibberish pass\n",
    "        # B.1 pass for original data\n",
    "        optimizer_diff.zero_grad()\n",
    "        out_orig_model_diff = model_diff(input_ids=input_ids,\n",
    "                                            token_type_ids=token_type_ids,\n",
    "                                            attention_mask=attention_mask,\n",
    "                                            labels=labels)\n",
    "        loss_orig_model_diff = out_orig_model_diff.loss\n",
    "        loss_orig_model_diff.backward()\n",
    "        orig_grads_model_diff = [p.grad.detach().cpu().clone() for p in model_diff.parameters()]\n",
    "\n",
    "        # B.2 pass for gibberish data\n",
    "        optimizer_diff.zero_grad()\n",
    "        gib_loss = torch.tensor(0.0, device=device)\n",
    "        for enc in gib_inputs:\n",
    "            out = model_diff(**enc, labels=labels)\n",
    "            gib_loss += out.loss\n",
    "        gib_loss /= num_variants\n",
    "        gib_loss.backward()\n",
    "        gib_grads = [p.grad.detach().cpu().clone() for p in model_diff.parameters()]\n",
    "\n",
    "        # C) Compute orthogonal vector\n",
    "        orig_vec_model_diff = parameters_to_vector(orig_grads_model_diff)\n",
    "        gib_vec = parameters_to_vector(gib_grads)\n",
    "        v_orth = gram_schmidt(gib_vec, orig_vec_model_diff)\n",
    "        #v_diff = orig_vec - orig_vec\n",
    "\n",
    "        # D) Update model_diff\n",
    "        optimizer_diff.zero_grad() # clear old grads\n",
    "        # unflatten v_orth → p.grad for each parameter p\n",
    "        pointer = 0\n",
    "        for p in model_diff.parameters():\n",
    "            if not p.requires_grad:\n",
    "                continue\n",
    "            numel = p.numel()\n",
    "            p.grad = v_orth[pointer:pointer+numel].view_as(p).to(device)\n",
    "            pointer += numel\n",
    "        optimizer_diff.step()\n",
    "\n",
    "\n",
    "        if batch_idx % 5 == 0:\n",
    "            print(f\"[Epoch {epoch+1} | Batch {batch_idx}] \"\n",
    "                  f\"orig_loss = {loss_orig.item():.4f}, gib_loss = {gib_loss.item():.4f}\")\n",
    "\n",
    "\n",
    "    print(f\"Epoch {epoch+1} complete\")\n",
    "\n",
    "model_orig.save_pretrained(\"trained_model_original_2\")\n",
    "model_diff.save_pretrained(\"trained_model_gradient_orth_2\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tilo_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
