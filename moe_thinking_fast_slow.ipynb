{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mixture of Experts (MoE) Implementation for \"Thinking Fast and Slow\"\n",
    "\n",
    "This notebook implements a Mixture of Experts (MoE) architecture to replicate key cognitive theories from Daniel Kahneman's \"Thinking Fast and Slow\". Unlike the previous CAV approach that manipulated existing model representations, this implementation creates specialized expert modules for different thinking styles and cognitive biases.\n",
    "\n",
    "## Overview\n",
    "\n",
    "The Mixture of Experts architecture consists of:\n",
    "1. Multiple expert networks (System 1, System 2, and various cognitive biases)\n",
    "2. A router network that decides which expert(s) to use for a given input\n",
    "3. A combiner that integrates outputs from multiple experts\n",
    "\n",
    "This approach is particularly innovative for modeling Kahneman's theories because:\n",
    "- It explicitly models the dual-process theory with separate neural pathways\n",
    "- It allows for dynamic switching between thinking styles based on context\n",
    "- It can model competition between System 1 and System 2 processes\n",
    "- It provides interpretable insights into when different thinking styles are activated\n",
    "\n",
    "Let's implement this architecture for replicating \"Thinking Fast and Slow\" theories."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup and Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: torch in /home/tsultanov/.local/lib/python3.10/site-packages (2.7.0)\n",
      "Requirement already satisfied: transformers in /home/tsultanov/.local/lib/python3.10/site-packages (4.51.3)\n",
      "Requirement already satisfied: datasets in /home/tsultanov/.local/lib/python3.10/site-packages (3.5.0)\n",
      "Requirement already satisfied: numpy in /home/tsultanov/.local/lib/python3.10/site-packages (2.2.5)\n",
      "Requirement already satisfied: matplotlib in /home/tsultanov/.local/lib/python3.10/site-packages (3.10.1)\n",
      "Requirement already satisfied: scikit-learn in /home/tsultanov/.local/lib/python3.10/site-packages (1.6.1)\n",
      "Requirement already satisfied: tqdm in /home/tsultanov/.local/lib/python3.10/site-packages (4.67.1)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch) (3.17.0)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in /home/tsultanov/.local/lib/python3.10/site-packages (from torch) (4.13.2)\n",
      "Requirement already satisfied: sympy>=1.13.3 in /home/tsultanov/.local/lib/python3.10/site-packages (from torch) (1.14.0)\n",
      "Requirement already satisfied: networkx in /home/tsultanov/.local/lib/python3.10/site-packages (from torch) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in /home/tsultanov/.local/lib/python3.10/site-packages (from torch) (3.1.6)\n",
      "Requirement already satisfied: fsspec in /home/tsultanov/.local/lib/python3.10/site-packages (from torch) (2024.12.0)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /home/tsultanov/.local/lib/python3.10/site-packages (from torch) (12.6.77)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /home/tsultanov/.local/lib/python3.10/site-packages (from torch) (12.6.77)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /home/tsultanov/.local/lib/python3.10/site-packages (from torch) (12.6.80)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.5.1.17 in /home/tsultanov/.local/lib/python3.10/site-packages (from torch) (9.5.1.17)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /home/tsultanov/.local/lib/python3.10/site-packages (from torch) (12.6.4.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /home/tsultanov/.local/lib/python3.10/site-packages (from torch) (11.3.0.4)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /home/tsultanov/.local/lib/python3.10/site-packages (from torch) (10.3.7.77)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /home/tsultanov/.local/lib/python3.10/site-packages (from torch) (11.7.1.2)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /home/tsultanov/.local/lib/python3.10/site-packages (from torch) (12.5.4.2)\n",
      "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.3 in /home/tsultanov/.local/lib/python3.10/site-packages (from torch) (0.6.3)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.26.2 in /home/tsultanov/.local/lib/python3.10/site-packages (from torch) (2.26.2)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /home/tsultanov/.local/lib/python3.10/site-packages (from torch) (12.6.77)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /home/tsultanov/.local/lib/python3.10/site-packages (from torch) (12.6.85)\n",
      "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /home/tsultanov/.local/lib/python3.10/site-packages (from torch) (1.11.1.6)\n",
      "Requirement already satisfied: triton==3.3.0 in /home/tsultanov/.local/lib/python3.10/site-packages (from torch) (3.3.0)\n",
      "Requirement already satisfied: setuptools>=40.8.0 in /usr/local/lib/python3.10/dist-packages (from triton==3.3.0->torch) (75.8.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.30.0 in /home/tsultanov/.local/lib/python3.10/site-packages (from transformers) (0.30.2)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/tsultanov/.local/lib/python3.10/site-packages (from transformers) (25.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /home/tsultanov/.local/lib/python3.10/site-packages (from transformers) (6.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /home/tsultanov/.local/lib/python3.10/site-packages (from transformers) (2024.11.6)\n",
      "Requirement already satisfied: requests in /home/tsultanov/.local/lib/python3.10/site-packages (from transformers) (2.32.3)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in /home/tsultanov/.local/lib/python3.10/site-packages (from transformers) (0.21.1)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in /home/tsultanov/.local/lib/python3.10/site-packages (from transformers) (0.5.3)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in /home/tsultanov/.local/lib/python3.10/site-packages (from datasets) (20.0.0)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /home/tsultanov/.local/lib/python3.10/site-packages (from datasets) (0.3.8)\n",
      "Requirement already satisfied: pandas in /home/tsultanov/.local/lib/python3.10/site-packages (from datasets) (2.2.3)\n",
      "Requirement already satisfied: xxhash in /home/tsultanov/.local/lib/python3.10/site-packages (from datasets) (3.5.0)\n",
      "Requirement already satisfied: multiprocess<0.70.17 in /home/tsultanov/.local/lib/python3.10/site-packages (from datasets) (0.70.16)\n",
      "Requirement already satisfied: aiohttp in /home/tsultanov/.local/lib/python3.10/site-packages (from datasets) (3.11.18)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /home/tsultanov/.local/lib/python3.10/site-packages (from matplotlib) (1.3.2)\n",
      "Requirement already satisfied: cycler>=0.10 in /home/tsultanov/.local/lib/python3.10/site-packages (from matplotlib) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /home/tsultanov/.local/lib/python3.10/site-packages (from matplotlib) (4.57.0)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in /home/tsultanov/.local/lib/python3.10/site-packages (from matplotlib) (1.4.8)\n",
      "Requirement already satisfied: pillow>=8 in /home/tsultanov/.local/lib/python3.10/site-packages (from matplotlib) (11.2.1)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /usr/lib/python3/dist-packages (from matplotlib) (2.4.7)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /home/tsultanov/.local/lib/python3.10/site-packages (from matplotlib) (2.9.0.post0)\n",
      "Requirement already satisfied: scipy>=1.6.0 in /home/tsultanov/.local/lib/python3.10/site-packages (from scikit-learn) (1.15.2)\n",
      "Requirement already satisfied: joblib>=1.2.0 in /home/tsultanov/.local/lib/python3.10/site-packages (from scikit-learn) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in /home/tsultanov/.local/lib/python3.10/site-packages (from scikit-learn) (3.6.0)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /home/tsultanov/.local/lib/python3.10/site-packages (from aiohttp->datasets) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /home/tsultanov/.local/lib/python3.10/site-packages (from aiohttp->datasets) (1.3.2)\n",
      "Requirement already satisfied: async-timeout<6.0,>=4.0 in /home/tsultanov/.local/lib/python3.10/site-packages (from aiohttp->datasets) (5.0.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /home/tsultanov/.local/lib/python3.10/site-packages (from aiohttp->datasets) (25.3.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /home/tsultanov/.local/lib/python3.10/site-packages (from aiohttp->datasets) (1.6.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /home/tsultanov/.local/lib/python3.10/site-packages (from aiohttp->datasets) (6.4.3)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /home/tsultanov/.local/lib/python3.10/site-packages (from aiohttp->datasets) (0.3.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /home/tsultanov/.local/lib/python3.10/site-packages (from aiohttp->datasets) (1.20.0)\n",
      "Requirement already satisfied: six>=1.5 in /usr/lib/python3/dist-packages (from python-dateutil>=2.7->matplotlib) (1.16.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/tsultanov/.local/lib/python3.10/site-packages (from requests->transformers) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/tsultanov/.local/lib/python3.10/site-packages (from requests->transformers) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/tsultanov/.local/lib/python3.10/site-packages (from requests->transformers) (2.4.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2024.12.14)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /home/tsultanov/.local/lib/python3.10/site-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /home/tsultanov/.local/lib/python3.10/site-packages (from jinja2->torch) (3.0.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /home/tsultanov/.local/lib/python3.10/site-packages (from pandas->datasets) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /home/tsultanov/.local/lib/python3.10/site-packages (from pandas->datasets) (2025.2)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# Apple Silicon compatibility settings\n",
    "import os\n",
    "os.environ[\"PYTORCH_ENABLE_MPS_FALLBACK\"] = \"1\"\n",
    "os.environ[\"PYTORCH_MPS_HIGH_WATERMARK_RATIO\"] = \"0.0\"\n",
    "os.environ[\"PYTORCH_NO_MPS\"] = \"1\"  # Completely disable MPS\n",
    "\n",
    "# Install required packages if needed\n",
    "!pip install torch transformers datasets numpy matplotlib scikit-learn tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu (forced for Apple Silicon compatibility)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, AutoConfig\n",
    "from transformers import GPT2LMHeadModel, GPT2Config\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tqdm.notebook import tqdm\n",
    "import json\n",
    "import random\n",
    "from typing import List, Dict, Tuple, Optional, Union, Any\n",
    "\n",
    "# Force CPU for all PyTorch operations\n",
    "device = torch.device(\"cpu\")\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "random.seed(42)\n",
    "np.random.seed(42)\n",
    "torch.manual_seed(42)\n",
    "\n",
    "print(f\"Using device: {device} (forced for Apple Silicon compatibility)\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Mixture of Experts Architecture\n",
    "\n",
    "We'll implement a Mixture of Experts architecture with the following components:\n",
    "1. Base language model (shared across experts)\n",
    "2. Expert-specific adapter layers for different thinking styles\n",
    "3. Router network to determine which expert to use\n",
    "4. Integration mechanism to combine expert outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ExpertAdapter(nn.Module):\n",
    "    \"\"\"\n",
    "    Adapter module for a specific expert (System 1, System 2, or cognitive bias).\n",
    "    Uses a bottleneck architecture for parameter efficiency.\n",
    "    \"\"\"\n",
    "    def __init__(self, hidden_size, bottleneck_size):\n",
    "        super().__init__()\n",
    "        self.down_project = nn.Linear(hidden_size, bottleneck_size)\n",
    "        self.activation = nn.GELU()\n",
    "        self.up_project = nn.Linear(bottleneck_size, hidden_size)\n",
    "        self.layer_norm = nn.LayerNorm(hidden_size)\n",
    "        \n",
    "    def forward(self, hidden_states):\n",
    "        residual = hidden_states\n",
    "        x = self.down_project(hidden_states)\n",
    "        x = self.activation(x)\n",
    "        x = self.up_project(x)\n",
    "        x = x + residual  # Residual connection\n",
    "        x = self.layer_norm(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RouterNetwork(nn.Module):\n",
    "    \"\"\"\n",
    "    Router network that decides which expert(s) to use for a given input.\n",
    "    \"\"\"\n",
    "    def __init__(self, hidden_size, num_experts, top_k=2):\n",
    "        super().__init__()\n",
    "        self.router = nn.Linear(hidden_size, num_experts)\n",
    "        self.top_k = top_k\n",
    "        \n",
    "    def forward(self, hidden_states):\n",
    "        # Average pooling over sequence dimension\n",
    "        pooled = hidden_states.mean(dim=1)  # [batch_size, hidden_size]\n",
    "        \n",
    "        # Calculate routing probabilities\n",
    "        routing_logits = self.router(pooled)  # [batch_size, num_experts]\n",
    "        routing_probs = F.softmax(routing_logits, dim=-1)\n",
    "        \n",
    "        # Select top-k experts\n",
    "        top_k_probs, top_k_indices = torch.topk(routing_probs, self.top_k, dim=-1)\n",
    "        \n",
    "        # Normalize the probabilities of selected experts\n",
    "        top_k_probs_normalized = top_k_probs / top_k_probs.sum(dim=-1, keepdim=True)\n",
    "        \n",
    "        return top_k_indices, top_k_probs_normalized, routing_probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ThinkingFastSlowMoE(nn.Module):\n",
    "    \"\"\"\n",
    "    Mixture of Experts implementation for \"Thinking Fast and Slow\" theories.\n",
    "    \"\"\"\n",
    "    def __init__(self, base_model, tokenizer, num_experts=5, bottleneck_size=256, top_k=2):\n",
    "        super().__init__()\n",
    "        self.base_model = base_model\n",
    "        self.tokenizer = tokenizer\n",
    "        self.hidden_size = base_model.config.hidden_size\n",
    "        self.num_experts = num_experts\n",
    "        self.top_k = top_k\n",
    "        \n",
    "        # Expert names and descriptions\n",
    "        self.expert_names = [\n",
    "            \"system1\",  # Fast, intuitive thinking\n",
    "            \"system2\",  # Slow, deliberate thinking\n",
    "            \"anchoring\",  # Anchoring bias\n",
    "            \"framing\",  # Framing effect\n",
    "            \"availability\"  # Availability heuristic\n",
    "        ]\n",
    "        \n",
    "        # Create expert adapters\n",
    "        self.experts = nn.ModuleList([\n",
    "            ExpertAdapter(self.hidden_size, bottleneck_size)\n",
    "            for _ in range(num_experts)\n",
    "        ])\n",
    "        \n",
    "        # Create router network\n",
    "        self.router = RouterNetwork(self.hidden_size, num_experts, top_k)\n",
    "        \n",
    "        # Final output projection\n",
    "        self.output_projection = nn.Linear(self.hidden_size, self.base_model.config.vocab_size)\n",
    "        \n",
    "        # Tie weights with base model's embedding\n",
    "        self.output_projection.weight = self.base_model.get_input_embeddings().weight\n",
    "        \n",
    "    def forward(self, input_ids, attention_mask=None, labels=None, return_expert_weights=False):\n",
    "        # Get base model hidden states (without final LM head)\n",
    "        outputs = self.base_model(input_ids, attention_mask=attention_mask, output_hidden_states=True)\n",
    "        hidden_states = outputs.hidden_states[-1]  # Last layer hidden states\n",
    "        \n",
    "        # Route to experts\n",
    "        expert_indices, expert_weights, all_routing_probs = self.router(hidden_states)\n",
    "        \n",
    "        # Initialize combined output\n",
    "        combined_output = torch.zeros_like(hidden_states)\n",
    "        \n",
    "        # Process through selected experts and combine outputs\n",
    "        for batch_idx in range(hidden_states.shape[0]):\n",
    "            for k in range(self.top_k):\n",
    "                expert_idx = expert_indices[batch_idx, k]\n",
    "                weight = expert_weights[batch_idx, k]\n",
    "                expert_output = self.experts[expert_idx](hidden_states[batch_idx:batch_idx+1])\n",
    "                combined_output[batch_idx:batch_idx+1] += weight * expert_output\n",
    "        \n",
    "        # Project to vocabulary\n",
    "        logits = self.output_projection(combined_output)\n",
    "        \n",
    "        # Calculate loss if labels are provided\n",
    "        loss = None\n",
    "        if labels is not None:\n",
    "            # Shift so that tokens < n predict n\n",
    "            shift_logits = logits[..., :-1, :].contiguous()\n",
    "            shift_labels = labels[..., 1:].contiguous()\n",
    "            loss_fct = nn.CrossEntropyLoss()\n",
    "            loss = loss_fct(shift_logits.view(-1, shift_logits.size(-1)), shift_labels.view(-1))\n",
    "        \n",
    "        if return_expert_weights:\n",
    "            return {\n",
    "                \"loss\": loss,\n",
    "                \"logits\": logits,\n",
    "                \"expert_indices\": expert_indices,\n",
    "                \"expert_weights\": expert_weights,\n",
    "                \"all_routing_probs\": all_routing_probs\n",
    "            }\n",
    "        else:\n",
    "            return {\n",
    "                \"loss\": loss,\n",
    "                \"logits\": logits\n",
    "            }\n",
    "    \n",
    "    def generate(self, input_ids, attention_mask=None, max_length=100, **kwargs):\n",
    "        \"\"\"\n",
    "        Generate text using the MoE model.\n",
    "        \"\"\"\n",
    "        # Start with input_ids\n",
    "        current_ids = input_ids.clone()\n",
    "        current_mask = attention_mask.clone() if attention_mask is not None else torch.ones_like(input_ids)\n",
    "        \n",
    "        # Keep track of which experts are used\n",
    "        expert_usage = []\n",
    "        \n",
    "        # Generate tokens one by one\n",
    "        for _ in range(max_length):\n",
    "            # Forward pass\n",
    "            outputs = self.forward(current_ids, attention_mask=current_mask, return_expert_weights=True)\n",
    "            \n",
    "            # Get next token logits (last token in sequence)\n",
    "            next_token_logits = outputs[\"logits\"][:, -1, :]\n",
    "            \n",
    "            # Sample next token\n",
    "            if \"temperature\" in kwargs:\n",
    "                next_token_logits = next_token_logits / kwargs[\"temperature\"]\n",
    "            \n",
    "            if \"top_p\" in kwargs:\n",
    "                # nucleus sampling\n",
    "                sorted_logits, sorted_indices = torch.sort(next_token_logits, descending=True)\n",
    "                cumulative_probs = torch.cumsum(F.softmax(sorted_logits, dim=-1), dim=-1)\n",
    "                sorted_indices_to_remove = cumulative_probs > kwargs[\"top_p\"]\n",
    "                sorted_indices_to_remove[..., 1:] = sorted_indices_to_remove[..., :-1].clone()\n",
    "                sorted_indices_to_remove[..., 0] = 0\n",
    "                indices_to_remove = sorted_indices_to_remove.scatter(1, sorted_indices, sorted_indices_to_remove)\n",
    "                next_token_logits = next_token_logits.masked_fill(indices_to_remove, -float(\"Inf\"))\n",
    "            \n",
    "            probs = F.softmax(next_token_logits, dim=-1)\n",
    "            next_token = torch.multinomial(probs, num_samples=1)\n",
    "            \n",
    "            # Record which experts were used for this token\n",
    "            batch_idx = 0  # Assuming batch size 1 for generation\n",
    "            top_experts = []\n",
    "            for k in range(self.top_k):\n",
    "                expert_idx = outputs[\"expert_indices\"][batch_idx, k].item()\n",
    "                weight = outputs[\"expert_weights\"][batch_idx, k].item()\n",
    "                top_experts.append((self.expert_names[expert_idx], weight))\n",
    "            expert_usage.append(top_experts)\n",
    "            \n",
    "            # Append to sequence\n",
    "            current_ids = torch.cat([current_ids, next_token], dim=1)\n",
    "            current_mask = torch.cat([current_mask, torch.ones_like(next_token)], dim=1)\n",
    "            \n",
    "            # Stop if EOS token is generated\n",
    "            if next_token[0, 0].item() == self.tokenizer.eos_token_id:\n",
    "                break\n",
    "        \n",
    "        return {\n",
    "            \"generated_ids\": current_ids,\n",
    "            \"expert_usage\": expert_usage\n",
    "        }\n",
    "    \n",
    "    def get_expert_usage_analysis(self, expert_usage):\n",
    "        \"\"\"\n",
    "        Analyze which experts were used during generation.\n",
    "        \"\"\"\n",
    "        # Count expert usage\n",
    "        expert_counts = {name: 0 for name in self.expert_names}\n",
    "        expert_weights = {name: 0.0 for name in self.expert_names}\n",
    "        \n",
    "        for token_experts in expert_usage:\n",
    "            for name, weight in token_experts:\n",
    "                expert_counts[name] += 1\n",
    "                expert_weights[name] += weight\n",
    "        \n",
    "        # Normalize\n",
    "        total_tokens = len(expert_usage)\n",
    "        for name in self.expert_names:\n",
    "            expert_counts[name] /= total_tokens\n",
    "            if expert_counts[name] > 0:\n",
    "                expert_weights[name] /= (expert_counts[name] * total_tokens)\n",
    "        \n",
    "        return {\n",
    "            \"usage_frequency\": expert_counts,\n",
    "            \"average_weight\": expert_weights\n",
    "        }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Data Preparation\n",
    "\n",
    "We'll prepare training data for each expert, with examples that exhibit the corresponding thinking style or cognitive bias."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_training_data():\n",
    "    \"\"\"\n",
    "    Prepare training data for each expert.\n",
    "    \"\"\"\n",
    "    training_data = {}\n",
    "    \n",
    "    # System 1 (Fast, Intuitive) Thinking\n",
    "    training_data[\"system1\"] = [\n",
    "        \"What is your gut feeling about this investment?\",\n",
    "        \"I immediately liked him when I saw his face.\",\n",
    "        \"This feels right to me, so I'll go with it.\",\n",
    "        \"My first impression is that this is a bad idea.\",\n",
    "        \"I don't need to analyze the data, I can tell it's going to work.\",\n",
    "        \"This reminds me of something that worked before, so let's do it.\",\n",
    "        \"I have a hunch this is the right decision.\",\n",
    "        \"That person looks trustworthy based on their appearance.\",\n",
    "        \"I'll make a quick decision without overthinking it.\",\n",
    "        \"This feels familiar, so it must be correct.\"\n",
    "    ]\n",
    "    \n",
    "    # System 2 (Slow, Deliberate) Thinking\n",
    "    training_data[\"system2\"] = [\n",
    "        \"Let's analyze the pros and cons of this investment systematically.\",\n",
    "        \"I need to calculate the expected value before making a decision.\",\n",
    "        \"What are the statistical probabilities of each outcome?\",\n",
    "        \"I should consider alternative explanations for this data.\",\n",
    "        \"Let me think step by step about the logical implications.\",\n",
    "        \"We need to evaluate the evidence objectively before concluding.\",\n",
    "        \"What cognitive biases might be affecting my judgment here?\",\n",
    "        \"I'll need to verify these assumptions with additional research.\",\n",
    "        \"The correlation doesn't necessarily imply causation in this case.\",\n",
    "        \"Let's construct a formal model to understand this problem.\"\n",
    "    ]\n",
    "    \n",
    "    # Anchoring Bias\n",
    "    training_data[\"anchoring\"] = [\n",
    "        \"The initial price was $1000. What do you think is a fair price?\",\n",
    "        \"The suggested donation amount is $50. How much would you like to donate?\",\n",
    "        \"The average score on this test is 85. What score do you expect to get?\",\n",
    "        \"This house was previously listed at $500,000. What would you offer?\",\n",
    "        \"The recommended daily steps are 10,000. How many steps do you think you should take?\",\n",
    "        \"The last bid was $200. What's your bid for this item?\",\n",
    "        \"Most people spend 2 hours on this task. How long do you think it will take you?\",\n",
    "        \"The speed limit here is 65 mph. How fast were you driving?\",\n",
    "        \"The standard tip is 20%. How much would you like to tip?\",\n",
    "        \"The CEO earns $5 million annually. What's a fair salary for the VP?\"\n",
    "    ]\n",
    "    \n",
    "    # Framing Effect\n",
    "    training_data[\"framing\"] = [\n",
    "        \"The treatment has a 70% success rate. Would you recommend it?\",\n",
    "        \"This investment has a 60% chance of making a profit. Is it worth it?\",\n",
    "        \"The program will save 200 out of 600 lives. Do you support it?\",\n",
    "        \"This policy will create jobs for 5% of the unemployed. Should we implement it?\",\n",
    "        \"The product has satisfied 90% of customers. Would you buy it?\",\n",
    "        \"This surgery has an 80% survival rate. Would you undergo it?\",\n",
    "        \"This diet plan helps 70% of people lose weight. Would you try it?\",\n",
    "        \"This security system prevents 75% of break-ins. Is it effective?\",\n",
    "        \"This vaccine protects 95% of recipients. Would you get vaccinated?\",\n",
    "        \"This educational program improves test scores for 65% of students. Is it valuable?\"\n",
    "    ]\n",
    "    \n",
    "    # Availability Heuristic\n",
    "    training_data[\"availability\"] = [\n",
    "        \"After hearing about a plane crash, how safe do you feel flying?\",\n",
    "        \"After reading about shark attacks, how dangerous do you think swimming in the ocean is?\",\n",
    "        \"After seeing news about lottery winners, how likely do you think winning the lottery is?\",\n",
    "        \"After hearing about a terrorist attack, how concerned are you about terrorism?\",\n",
    "        \"After reading about a rare disease, how worried are you about contracting it?\",\n",
    "        \"After watching a documentary about serial killers, how safe do you feel walking alone?\",\n",
    "        \"After hearing about a friend's divorce, how stable do you think marriages are?\",\n",
    "        \"After reading about a stock market crash, how risky do you think investing is?\",\n",
    "        \"After seeing news about car accidents, how dangerous do you think driving is?\",\n",
    "        \"After hearing about a home invasion, how concerned are you about your home security?\"\n",
    "    ]\n",
    "    \n",
    "    return training_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_test_prompts():\n",
    "    \"\"\"\n",
    "    Prepare test prompts to evaluate the MoE model.\n",
    "    \"\"\"\n",
    "    test_prompts = [\n",
    "        # General decision-making prompts\n",
    "        \"Should I invest in this new technology company?\",\n",
    "        \"Is this a good time to buy a house?\",\n",
    "        \"How should I approach this difficult conversation?\",\n",
    "        \n",
    "        # Anchoring bias prompts\n",
    "        \"The suggested price is $1200. What do you think is a fair price for this laptop?\",\n",
    "        \"Most people donate $50. How much would you like to donate?\",\n",
    "        \n",
    "        # Framing effect prompts\n",
    "        \"This treatment has a 70% success rate. Would you recommend it?\",\n",
    "        \"This treatment has a 30% failure rate. Would you recommend it?\",\n",
    "        \n",
    "        # Availability heuristic prompts\n",
    "        \"After reading about a plane crash yesterday, how safe do you think flying is?\",\n",
    "        \"How likely do you think it is to be attacked by a shark while swimming?\"\n",
    "    ]\n",
    "    \n",
    "    return test_prompts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Model Initialization and Training\n",
    "\n",
    "We'll initialize our MoE model with a pre-trained language model and train it on our expert-specific data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading gpt2...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "486fef182fd34419b62c1703d4728ff1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/26.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/terlan/anaconda3/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d2b307e753eb430f9a5d7a6983a60f7f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/665 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "648ea565b7bd4fd5b83e5422f7a43458",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json:   0%|          | 0.00/1.04M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9dea0100eb2942389406e634d2df20ac",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "269b429b9e9e4748aa36b73bbef9cd63",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aee3ed8a25a343bf97a7a3688e27a8eb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/548M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "15a999bd5b6940c6934eb9b02a710eb2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/124 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded successfully!\n"
     ]
    }
   ],
   "source": [
    "# Load a smaller pre-trained model\n",
    "model_name = \"gpt2\"  # Smaller model for faster training and compatibility\n",
    "print(f\"Loading {model_name}...\")\n",
    "\n",
    "# Load model and tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "base_model = AutoModelForCausalLM.from_pretrained(model_name)\n",
    "\n",
    "# Ensure model is on CPU for Apple Silicon compatibility\n",
    "base_model = base_model.to(\"cpu\")\n",
    "\n",
    "# Add padding token if it doesn't exist\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "print(\"Model loaded successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize MoE model\n",
    "moe_model = ThinkingFastSlowMoE(\n",
    "    base_model=base_model,\n",
    "    tokenizer=tokenizer,\n",
    "    num_experts=5,  # system1, system2, anchoring, framing, availability\n",
    "    bottleneck_size=128,\n",
    "    top_k=2  # Use top 2 experts for each input\n",
    ")\n",
    "\n",
    "# Ensure model is on CPU\n",
    "moe_model = moe_model.to(\"cpu\")\n",
    "\n",
    "print(f\"Initialized MoE model with {len(moe_model.expert_names)} experts: {moe_model.expert_names}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_moe_model(model, training_data, tokenizer, num_epochs=3, batch_size=2, learning_rate=5e-5):\n",
    "    \"\"\"\n",
    "    Train the MoE model on expert-specific data.\n",
    "    \"\"\"\n",
    "    # Prepare optimizer\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
    "    \n",
    "    # Prepare training examples\n",
    "    all_examples = []\n",
    "    all_expert_labels = []\n",
    "    \n",
    "    for expert_idx, expert_name in enumerate(model.expert_names):\n",
    "        if expert_name in training_data:\n",
    "            for example in training_data[expert_name]:\n",
    "                all_examples.append(example)\n",
    "                all_expert_labels.append(expert_idx)\n",
    "    \n",
    "    # Training loop\n",
    "    model.train()\n",
    "    for epoch in range(num_epochs):\n",
    "        print(f\"Epoch {epoch+1}/{num_epochs}\")\n",
    "        \n",
    "        # Shuffle data\n",
    "        indices = list(range(len(all_examples)))\n",
    "        random.shuffle(indices)\n",
    "        \n",
    "        # Process in batches\n",
    "        total_loss = 0\n",
    "        num_batches = (len(indices) + batch_size - 1) // batch_size\n",
    "        \n",
    "        for batch_idx in tqdm(range(num_batches)):\n",
    "            # Get batch indices\n",
    "            batch_start = batch_idx * batch_size\n",
    "            batch_end = min((batch_idx + 1) * batch_size, len(indices))\n",
    "            batch_indices = indices[batch_start:batch_end]\n",
    "            \n",
    "            # Prepare batch\n",
    "            batch_examples = [all_examples[i] for i in batch_indices]\n",
    "            batch_expert_labels = [all_expert_labels[i] for i in batch_indices]\n",
    "            \n",
    "            # Tokenize\n",
    "            encodings = tokenizer(batch_examples, padding=True, truncation=True, return_tensors=\"pt\")\n",
    "            input_ids = encodings.input_ids\n",
    "            attention_mask = encodings.attention_mask\n",
    "            \n",
    "            # Forward pass\n",
    "            outputs = model(input_ids, attention_mask=attention_mask, labels=input_ids)\n",
    "            loss = outputs[\"loss\"]\n",
    "            \n",
    "            # Add routing loss to encourage using the correct expert\n",
    "            if \"all_routing_probs\" in outputs:\n",
    "                routing_probs = outputs[\"all_routing_probs\"]\n",
    "                expert_labels = torch.tensor(batch_expert_labels, device=routing_probs.device)\n",
    "                routing_loss = F.cross_entropy(routing_probs, expert_labels)\n",
    "                loss = loss + 0.1 * routing_loss  # Weight for routing loss\n",
    "            \n",
    "            # Backward pass and optimize\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            total_loss += loss.item()\n",
    "        \n",
    "        avg_loss = total_loss / num_batches\n",
    "        print(f\"Average loss: {avg_loss:.4f}\")\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare training data\n",
    "training_data = prepare_training_data()\n",
    "\n",
    "# Train the MoE model\n",
    "# Note: This is a simplified training loop for demonstration\n",
    "# In a real implementation, you would use more data and more epochs\n",
    "trained_moe_model = train_moe_model(\n",
    "    model=moe_model,\n",
    "    training_data=training_data,\n",
    "    tokenizer=tokenizer,\n",
    "    num_epochs=3,\n",
    "    batch_size=2,\n",
    "    learning_rate=5e-5\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Testing and Evaluation\n",
    "\n",
    "Now let's test our MoE model on various prompts and analyze which experts are activated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_moe_model(model, tokenizer, test_prompts):\n",
    "    \"\"\"\n",
    "    Test the MoE model on various prompts and analyze expert usage.\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    results = []\n",
    "    \n",
    "    for prompt in test_prompts:\n",
    "        print(f\"\\nPrompt: {prompt}\")\n",
    "        \n",
    "        # Tokenize\n",
    "        input_ids = tokenizer(prompt, return_tensors=\"pt\").input_ids\n",
    "        attention_mask = torch.ones_like(input_ids)\n",
    "        \n",
    "        # Generate\n",
    "        with torch.no_grad():\n",
    "            outputs = model.generate(\n",
    "                input_ids=input_ids,\n",
    "                attention_mask=attention_mask,\n",
    "                max_length=50,\n",
    "                temperature=0.7,\n",
    "                top_p=0.9\n",
    "            )\n",
    "        \n",
    "        # Decode generated text\n",
    "        generated_ids = outputs[\"generated_ids\"]\n",
    "        generated_text = tokenizer.decode(generated_ids[0], skip_special_tokens=True)\n",
    "        print(f\"Generated: {generated_text}\")\n",
    "        \n",
    "        # Analyze expert usage\n",
    "        expert_usage = outputs[\"expert_usage\"]\n",
    "        analysis = model.get_expert_usage_analysis(expert_usage)\n",
    "        \n",
    "        print(\"\\nExpert Usage:\")\n",
    "        for expert_name in model.expert_names:\n",
    "            freq = analysis[\"usage_frequency\"][expert_name]\n",
    "            weight = analysis[\"average_weight\"][expert_name]\n",
    "            print(f\"  {expert_name}: {freq:.2f} frequency, {weight:.2f} average weight\")\n",
    "        \n",
    "        # Store results\n",
    "        results.append({\n",
    "            \"prompt\": prompt,\n",
    "            \"generated_text\": generated_text,\n",
    "            \"expert_usage\": analysis\n",
    "        })\n",
    "        \n",
    "        print(\"-\" * 80)\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare test prompts\n",
    "test_prompts = prepare_test_prompts()\n",
    "\n",
    "# Test the MoE model\n",
    "test_results = test_moe_model(trained_moe_model, tokenizer, test_prompts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Visualization and Analysis\n",
    "\n",
    "Let's visualize the expert usage patterns to better understand how the MoE model is working."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_expert_usage(results, expert_names):\n",
    "    \"\"\"\n",
    "    Visualize expert usage across different prompts.\n",
    "    \"\"\"\n",
    "    # Prepare data for visualization\n",
    "    prompts = [r[\"prompt\"][:30] + \"...\" if len(r[\"prompt\"]) > 30 else r[\"prompt\"] for r in results]\n",
    "    expert_data = {}\n",
    "    \n",
    "    for expert_name in expert_names:\n",
    "        expert_data[expert_name] = [r[\"expert_usage\"][\"usage_frequency\"][expert_name] for r in results]\n",
    "    \n",
    "    # Create stacked bar chart\n",
    "    fig, ax = plt.subplots(figsize=(12, 8))\n",
    "    bottom = np.zeros(len(prompts))\n",
    "    \n",
    "    for expert_name in expert_names:\n",
    "        ax.bar(prompts, expert_data[expert_name], bottom=bottom, label=expert_name)\n",
    "        bottom += np.array(expert_data[expert_name])\n",
    "    \n",
    "    ax.set_title(\"Expert Usage Across Different Prompts\")\n",
    "    ax.set_xlabel(\"Prompts\")\n",
    "    ax.set_ylabel(\"Usage Frequency\")\n",
    "    ax.legend()\n",
    "    plt.xticks(rotation=45, ha=\"right\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Create heatmap of expert usage\n",
    "    fig, ax = plt.subplots(figsize=(12, 8))\n",
    "    data = np.array([expert_data[name] for name in expert_names])\n",
    "    im = ax.imshow(data, cmap=\"YlOrRd\")\n",
    "    \n",
    "    # Add labels\n",
    "    ax.set_xticks(np.arange(len(prompts)))\n",
    "    ax.set_yticks(np.arange(len(expert_names)))\n",
    "    ax.set_xticklabels(prompts)\n",
    "    ax.set_yticklabels(expert_names)\n",
    "    plt.setp(ax.get_xticklabels(), rotation=45, ha=\"right\", rotation_mode=\"anchor\")\n",
    "    \n",
    "    # Add colorbar\n",
    "    cbar = ax.figure.colorbar(im, ax=ax)\n",
    "    cbar.ax.set_ylabel(\"Usage Frequency\", rotation=-90, va=\"bottom\")\n",
    "    \n",
    "    ax.set_title(\"Expert Usage Heatmap\")\n",
    "    fig.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize expert usage\n",
    "visualize_expert_usage(test_results, trained_moe_model.expert_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Comparison with Base Model\n",
    "\n",
    "Let's compare the outputs of our MoE model with the base model to see how the expert specialization affects the responses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_with_base_model(base_model, tokenizer, moe_model, test_prompts):\n",
    "    \"\"\"\n",
    "    Compare outputs from the base model and MoE model.\n",
    "    \"\"\"\n",
    "    base_model.eval()\n",
    "    moe_model.eval()\n",
    "    \n",
    "    for prompt in test_prompts:\n",
    "        print(f\"\\nPrompt: {prompt}\")\n",
    "        \n",
    "        # Tokenize\n",
    "        input_ids = tokenizer(prompt, return_tensors=\"pt\").input_ids\n",
    "        \n",
    "        # Generate with base model\n",
    "        with torch.no_grad():\n",
    "            base_outputs = base_model.generate(\n",
    "                input_ids,\n",
    "                max_length=input_ids.shape[1] + 50,\n",
    "                temperature=0.7,\n",
    "                top_p=0.9\n",
    "            )\n",
    "        \n",
    "        base_text = tokenizer.decode(base_outputs[0], skip_special_tokens=True)\n",
    "        \n",
    "        # Generate with MoE model\n",
    "        with torch.no_grad():\n",
    "            moe_outputs = moe_model.generate(\n",
    "                input_ids=input_ids,\n",
    "                attention_mask=torch.ones_like(input_ids),\n",
    "                max_length=50,\n",
    "                temperature=0.7,\n",
    "                top_p=0.9\n",
    "            )\n",
    "        \n",
    "        moe_text = tokenizer.decode(moe_outputs[\"generated_ids\"][0], skip_special_tokens=True)\n",
    "        \n",
    "        # Get expert usage\n",
    "        expert_usage = moe_outputs[\"expert_usage\"]\n",
    "        analysis = moe_model.get_expert_usage_analysis(expert_usage)\n",
    "        \n",
    "        # Find dominant expert\n",
    "        dominant_expert = max(analysis[\"usage_frequency\"].items(), key=lambda x: x[1])[0]\n",
    "        \n",
    "        print(f\"\\nBase Model Output:\\n{base_text}\")\n",
    "        print(f\"\\nMoE Model Output (dominant expert: {dominant_expert}):\\n{moe_text}\")\n",
    "        print(\"-\" * 80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare with base model\n",
    "# Select a subset of test prompts for comparison\n",
    "comparison_prompts = test_prompts[:3]\n",
    "compare_with_base_model(base_model, tokenizer, trained_moe_model, comparison_prompts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Forcing Specific Experts\n",
    "\n",
    "Let's implement a way to force the model to use specific experts, allowing us to explicitly control the thinking style."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_with_forced_expert(model, tokenizer, prompt, expert_name, weight=0.9):\n",
    "    \"\"\"\n",
    "    Generate text with a forced expert to control thinking style.\n",
    "    \"\"\"\n",
    "    if expert_name not in model.expert_names:\n",
    "        raise ValueError(f\"Unknown expert: {expert_name}. Available experts: {model.expert_names}\")\n",
    "    \n",
    "    expert_idx = model.expert_names.index(expert_name)\n",
    "    \n",
    "    # Create a custom router class that overrides the forward method\n",
    "    original_forward = model.router.forward\n",
    "    \n",
    "    def forced_router_forward(hidden_states):\n",
    "        batch_size = hidden_states.shape[0]\n",
    "        top_k = model.top_k\n",
    "        \n",
    "        # Create indices tensor with the forced expert as the first choice\n",
    "        indices = torch.zeros((batch_size, top_k), dtype=torch.long, device=hidden_states.device)\n",
    "        indices[:, 0] = expert_idx\n",
    "        \n",
    "        # For the remaining slots, use other experts\n",
    "        other_experts = [i for i in range(len(model.expert_names)) if i != expert_idx]\n",
    "        for k in range(1, top_k):\n",
    "            if k-1 < len(other_experts):\n",
    "                indices[:, k] = other_experts[k-1]\n",
    "            else:\n",
    "                indices[:, k] = 0  # Default to first expert if we run out\n",
    "        \n",
    "        # Create weights tensor with high weight for the forced expert\n",
    "        weights = torch.zeros((batch_size, top_k), dtype=torch.float, device=hidden_states.device)\n",
    "        weights[:, 0] = weight  # High weight for forced expert\n",
    "        remaining_weight = 1.0 - weight\n",
    "        for k in range(1, top_k):\n",
    "            weights[:, k] = remaining_weight / (top_k - 1)\n",
    "        \n",
    "        # Run original forward to get all routing probs for analysis\n",
    "        _, _, all_routing_probs = original_forward(hidden_states)\n",
    "        \n",
    "        return indices, weights, all_routing_probs\n",
    "    \n",
    "    # Replace router forward method temporarily\n",
    "    model.router.forward = forced_router_forward\n",
    "    \n",
    "    # Generate text\n",
    "    input_ids = tokenizer(prompt, return_tensors=\"pt\").input_ids\n",
    "    attention_mask = torch.ones_like(input_ids)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            max_length=50,\n",
    "            temperature=0.7,\n",
    "            top_p=0.9\n",
    "        )\n",
    "    \n",
    "    # Restore original router forward method\n",
    "    model.router.forward = original_forward\n",
    "    \n",
    "    # Decode generated text\n",
    "    generated_ids = outputs[\"generated_ids\"]\n",
    "    generated_text = tokenizer.decode(generated_ids[0], skip_special_tokens=True)\n",
    "    \n",
    "    return generated_text, outputs[\"expert_usage\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_forced_experts(model, tokenizer, prompt):\n",
    "    \"\"\"\n",
    "    Test generating with different forced experts for the same prompt.\n",
    "    \"\"\"\n",
    "    print(f\"Prompt: {prompt}\\n\")\n",
    "    \n",
    "    for expert_name in model.expert_names:\n",
    "        print(f\"\\nForced Expert: {expert_name}\")\n",
    "        generated_text, expert_usage = generate_with_forced_expert(\n",
    "            model=model,\n",
    "            tokenizer=tokenizer,\n",
    "            prompt=prompt,\n",
    "            expert_name=expert_name,\n",
    "            weight=0.9\n",
    "        )\n",
    "        \n",
    "        print(f\"Generated: {generated_text}\")\n",
    "        \n",
    "        # Analyze expert usage\n",
    "        analysis = model.get_expert_usage_analysis(expert_usage)\n",
    "        print(\"\\nExpert Usage:\")\n",
    "        for name in model.expert_names:\n",
    "            freq = analysis[\"usage_frequency\"][name]\n",
    "            weight = analysis[\"average_weight\"][name]\n",
    "            print(f\"  {name}: {freq:.2f} frequency, {weight:.2f} average weight\")\n",
    "        \n",
    "        print(\"-\" * 40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test with forced experts\n",
    "test_prompt = \"Should I make this important decision now or wait until tomorrow?\"\n",
    "test_forced_experts(trained_moe_model, tokenizer, test_prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Saving and Loading the Model\n",
    "\n",
    "Let's implement functions to save and load our MoE model for future use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_moe_model(model, tokenizer, save_dir):\n",
    "    \"\"\"\n",
    "    Save the MoE model and tokenizer.\n",
    "    \"\"\"\n",
    "    # Create directory if it doesn't exist\n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "    \n",
    "    # Save base model and tokenizer\n",
    "    model.base_model.save_pretrained(os.path.join(save_dir, \"base_model\"))\n",
    "    tokenizer.save_pretrained(os.path.join(save_dir, \"tokenizer\"))\n",
    "    \n",
    "    # Save expert adapters and router\n",
    "    torch.save(model.experts.state_dict(), os.path.join(save_dir, \"experts.pt\"))\n",
    "    torch.save(model.router.state_dict(), os.path.join(save_dir, \"router.pt\"))\n",
    "    torch.save(model.output_projection.state_dict(), os.path.join(save_dir, \"output_projection.pt\"))\n",
    "    \n",
    "    # Save configuration\n",
    "    config = {\n",
    "        \"num_experts\": model.num_experts,\n",
    "        \"top_k\": model.top_k,\n",
    "        \"hidden_size\": model.hidden_size,\n",
    "        \"expert_names\": model.expert_names,\n",
    "        \"bottleneck_size\": model.experts[0].down_project.out_features\n",
    "    }\n",
    "    \n",
    "    with open(os.path.join(save_dir, \"config.json\"), \"w\") as f:\n",
    "        json.dump(config, f)\n",
    "    \n",
    "    print(f\"Model saved to {save_dir}\")\n",
    "\n",
    "def load_moe_model(save_dir):\n",
    "    \"\"\"\n",
    "    Load the MoE model and tokenizer.\n",
    "    \"\"\"\n",
    "    # Load configuration\n",
    "    with open(os.path.join(save_dir, \"config.json\"), \"r\") as f:\n",
    "        config = json.load(f)\n",
    "    \n",
    "    # Load base model and tokenizer\n",
    "    base_model = AutoModelForCausalLM.from_pretrained(os.path.join(save_dir, \"base_model\"))\n",
    "    tokenizer = AutoTokenizer.from_pretrained(os.path.join(save_dir, \"tokenizer\"))\n",
    "    \n",
    "    # Initialize MoE model\n",
    "    model = ThinkingFastSlowMoE(\n",
    "        base_model=base_model,\n",
    "        tokenizer=tokenizer,\n",
    "        num_experts=config[\"num_experts\"],\n",
    "        bottleneck_size=config[\"bottleneck_size\"],\n",
    "        top_k=config[\"top_k\"]\n",
    "    )\n",
    "    \n",
    "    # Override expert names\n",
    "    model.expert_names = config[\"expert_names\"]\n",
    "    \n",
    "    # Load expert adapters and router\n",
    "    model.experts.load_state_dict(torch.load(os.path.join(save_dir, \"experts.pt\")))\n",
    "    model.router.load_state_dict(torch.load(os.path.join(save_dir, \"router.pt\")))\n",
    "    model.output_projection.load_state_dict(torch.load(os.path.join(save_dir, \"output_projection.pt\")))\n",
    "    \n",
    "    print(f\"Model loaded from {save_dir}\")\n",
    "    return model, tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the model\n",
    "save_dir = \"thinking_fast_slow_moe_model\"\n",
    "save_moe_model(trained_moe_model, tokenizer, save_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the model (commented out to avoid reloading unnecessarily)\n",
    "# loaded_model, loaded_tokenizer = load_moe_model(save_dir)\n",
    "# test_forced_experts(loaded_model, loaded_tokenizer, test_prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Conclusion and Comparison with CAV Approach\n",
    "\n",
    "Let's compare the Mixture of Experts approach with the previous Concept Activation Vectors approach."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparison of MoE vs. CAV Approaches\n",
    "\n",
    "| Aspect | Mixture of Experts (MoE) | Concept Activation Vectors (CAV) |\n",
    "|--------|--------------------------|----------------------------------|\n",
    "| **Core Mechanism** | Uses separate expert modules for different thinking styles | Identifies and manipulates directions in activation space |\n",
    "| **Architecture** | Modifies model architecture with expert-specific adapters | Keeps model architecture unchanged, manipulates activations |\n",
    "| **Training** | Requires training the expert adapters and router | Only trains linear classifiers to identify concept directions |\n",
    "| **Interpretability** | Explicit separation of thinking styles into different modules | Implicit representation of thinking styles as directions |\n",
    "| **Control** | Can force specific experts to control thinking style | Can control steering strength to enhance/suppress concepts |\n",
    "| **Computational Cost** | Higher (multiple expert modules) | Lower (only adds concept vectors during inference) |\n",
    "| **Theoretical Alignment** | Directly models dual-process theory with separate pathways | Represents concepts as continuous directions in semantic space |\n",
    "\n",
    "### Advantages of MoE for \"Thinking Fast and Slow\"\n",
    "\n",
    "1. **Explicit Dual-Process Modeling**: MoE directly implements Kahneman's dual-process theory by having separate expert modules for System 1 and System 2 thinking.\n",
    "\n",
    "2. **Dynamic Expert Selection**: The router network dynamically decides which thinking style to use based on the input, similar to how humans switch between intuitive and analytical thinking.\n",
    "\n",
    "3. **Competitive Processing**: MoE can model the competition between System 1 and System 2 processes, with the router determining which expert gets more weight.\n",
    "\n",
    "4. **Interpretable Expert Usage**: We can analyze which experts are activated for different inputs, providing insights into when different thinking styles are used.\n",
    "\n",
    "5. **Modular Architecture**: Easy to add new experts for additional cognitive biases or thinking styles.\n",
    "\n",
    "### Limitations and Future Work\n",
    "\n",
    "1. **Training Data Requirements**: MoE requires more training data and computation than CAV.\n",
    "\n",
    "2. **Architectural Complexity**: More complex architecture with multiple expert modules.\n",
    "\n",
    "3. **Future Extensions**:\n",
    "   - Implement more sophisticated routing mechanisms\n",
    "   - Add experts for additional cognitive biases\n",
    "   - Combine MoE with CAV for even more control\n",
    "   - Evaluate on more complex decision-making tasks\n",
    "\n",
    "This Mixture of Experts implementation provides a novel and theoretically grounded approach to modeling Kahneman's theories in language models, making it an excellent choice for a master's thesis on assessing deep learning architectures for learning expertise."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
