{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bff1e2c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from pathlib import Path\n",
    "import torch\n",
    "import os\n",
    "import gc\n",
    "from collections import Counter\n",
    "from torch.utils.data import Dataset, random_split\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, Trainer, TrainingArguments\n",
    "from peft import PeftModel\n",
    "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "804a7fe5",
   "metadata": {},
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a1717b72",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_legitimate_features(responses):\n",
    "    \"\"\"\n",
    "    Extract only legitimate, non-leaky features from the responses.\n",
    "    \"\"\"\n",
    "    features = {}\n",
    "    \n",
    "    # Helper to extract response safely\n",
    "    def extract_response(responses, code):\n",
    "        for r in responses:\n",
    "            if r[\"variable_code\"] == code:\n",
    "                ans = r.get(\"respondent_answer\")\n",
    "                if ans in ['Inapplicable', 'Refused', \"Don't know\", 'Error']:\n",
    "                    return \"NA\"\n",
    "                return str(ans)\n",
    "        return \"NA\"\n",
    "    \n",
    "    # Demographic features\n",
    "    features[\"political_interest\"] = extract_response(responses, \"V241004\")  # Political interest\n",
    "    features[\"campaign_interest\"] = extract_response(responses, \"V241005\")   # Campaign interest\n",
    "    \n",
    "    # Economic views (if available)\n",
    "    features[\"economic_views\"] = extract_response(responses, \"V241127\")\n",
    "    \n",
    "    # State/region information\n",
    "    features[\"state\"] = extract_response(responses, \"V241017\")\n",
    "    \n",
    "    # Media consumption (example)\n",
    "    features[\"media_consumption\"] = extract_response(responses, \"V241201\")\n",
    "    \n",
    "    # Convert features to a single text representation\n",
    "    input_text = (\n",
    "        f\"Political interest: {features['political_interest']}\\n\"\n",
    "        f\"Campaign interest: {features['campaign_interest']}\\n\"\n",
    "        f\"Economic views: {features['economic_views']}\\n\"\n",
    "        f\"State: {features['state']}\\n\"\n",
    "        f\"Media consumption: {features['media_consumption']}\\n\"\n",
    "        f\"Q: Who would this respondent vote for in a Harris vs Trump election?\"\n",
    "    )\n",
    "    \n",
    "    return input_text, features\n",
    "\n",
    "def load_data(data_folder, variable_code, exclude_classes=None, include_classes=None):\n",
    "    \"\"\"\n",
    "    Loads question-response pairs for a given ANES variable code.\n",
    "    Uses only legitimate features that don't leak the outcome.\n",
    "    \"\"\"\n",
    "    examples = []\n",
    "    label_map = {}\n",
    "    next_label_id = 0\n",
    "    features_data = []\n",
    "\n",
    "    excluded_count = 0\n",
    "    included_count = 0\n",
    "    missing_answer_count = 0\n",
    "    not_included_count = 0\n",
    "    matched_count = 0\n",
    "\n",
    "    if exclude_classes is None:\n",
    "        exclude_classes = ['Inapplicable', 'Refused', \"Don't know\", 'Error', \"Don't know\"]\n",
    "\n",
    "    json_files = [f for f in os.listdir(data_folder) if f.endswith('.json')]\n",
    "    print(f\"Processing {len(json_files)} JSON files for variable {variable_code}\")\n",
    "\n",
    "    for i, fname in enumerate(json_files):\n",
    "        if i % 500 == 0:\n",
    "            print(f\"Progress: {i}/{len(json_files)} files processed\")\n",
    "\n",
    "        \"\"\" try:\n",
    "            with open(os.path.join(data_folder, fname)) as f:\n",
    "                respondent = json.load(f)\n",
    "        except (json.JSONDecodeError, FileNotFoundError):\n",
    "            continue\n",
    "\n",
    "        responses = respondent.get(\"responses\", []) \"\"\"\n",
    "        with open(os.path.join(data_folder, fname)) as f:\n",
    "            try:\n",
    "                data = json.load(f)\n",
    "                if isinstance(data, dict):\n",
    "                    respondents = [data]\n",
    "                elif isinstance(data, list):\n",
    "                    respondents = data\n",
    "                else:\n",
    "                    continue\n",
    "            except (json.JSONDecodeError, FileNotFoundError):\n",
    "                continue\n",
    "\n",
    "            for respondent in respondents:\n",
    "                responses = respondent.get(\"responses\", [])\n",
    "                found = False\n",
    "                for item in responses:\n",
    "                    if item.get(\"variable_code\") != variable_code:\n",
    "                        continue\n",
    "\n",
    "                    question = item.get(\"full_question_text\", \"\")\n",
    "                    possible_answers = [opt[\"text\"] for opt in item.get(\"possible_answers\", [])]\n",
    "                    respondent_answer = item.get(\"respondent_answer\", None)\n",
    "\n",
    "                    if not respondent_answer:\n",
    "                        missing_answer_count += 1\n",
    "                        continue\n",
    "\n",
    "                    if respondent_answer in exclude_classes:\n",
    "                        excluded_count += 1\n",
    "                        continue\n",
    "\n",
    "                    if include_classes and respondent_answer not in include_classes:\n",
    "                        not_included_count += 1\n",
    "                        continue\n",
    "\n",
    "                    included_count += 1\n",
    "\n",
    "                    if respondent_answer not in label_map:\n",
    "                        label_map[respondent_answer] = next_label_id\n",
    "                        next_label_id += 1\n",
    "                    label = label_map[respondent_answer]\n",
    "\n",
    "                    input_text, features = extract_legitimate_features(responses)\n",
    "\n",
    "                    examples.append((input_text, label))\n",
    "                    features_data.append(features)\n",
    "                    matched_count += 1\n",
    "                    found = True\n",
    "                    break  # Only use first match per respondent\n",
    "\n",
    "        found = False\n",
    "        for item in responses:\n",
    "            if item.get(\"variable_code\") != variable_code:\n",
    "                continue\n",
    "\n",
    "            question = item.get(\"full_question_text\", \"\")\n",
    "            possible_answers = [opt[\"text\"] for opt in item.get(\"possible_answers\", [])]\n",
    "            respondent_answer = item.get(\"respondent_answer\", None)\n",
    "\n",
    "            if not respondent_answer:\n",
    "                missing_answer_count += 1\n",
    "                continue\n",
    "\n",
    "            if respondent_answer in exclude_classes:\n",
    "                excluded_count += 1\n",
    "                continue\n",
    "\n",
    "            if include_classes and respondent_answer not in include_classes:\n",
    "                not_included_count += 1\n",
    "                continue\n",
    "\n",
    "            included_count += 1\n",
    "\n",
    "            if respondent_answer not in label_map:\n",
    "                label_map[respondent_answer] = next_label_id\n",
    "                next_label_id += 1\n",
    "            label = label_map[respondent_answer]\n",
    "\n",
    "            # Extract legitimate features instead of leaky ones\n",
    "            input_text, features = extract_legitimate_features(responses)\n",
    "            \n",
    "            examples.append((input_text, label))\n",
    "            features_data.append(features)\n",
    "            matched_count += 1\n",
    "            found = True\n",
    "            break  # Only use first match per respondent\n",
    "\n",
    "    # Summary logging\n",
    "    print(f\"\\nüìä Summary for variable {variable_code}:\")\n",
    "    print(f\"  ‚û§ Total JSON files: {len(json_files)}\")\n",
    "    print(f\"  ‚û§ Valid examples collected: {matched_count}\")\n",
    "    print(f\"  ‚û§ Unique labels: {len(label_map)}\")\n",
    "    print(f\"  ‚û§ Skipped due to missing answers: {missing_answer_count}\")\n",
    "    print(f\"  ‚û§ Skipped due to exclusion list: {excluded_count}\")\n",
    "    print(f\"  ‚û§ Skipped (not in include_classes): {not_included_count}\")\n",
    "    if include_classes:\n",
    "        print(f\"  ‚û§ Included only: {include_classes}\")\n",
    "    print(f\"  ‚û§ Final label map: {label_map}\")\n",
    "\n",
    "    # Class distribution\n",
    "    label_counts = Counter([label for _, label in examples])\n",
    "    print(\"\\nüîç Class distribution (label IDs):\", label_counts)\n",
    "    for label, count in label_counts.items():\n",
    "        for key, val in label_map.items():\n",
    "            if val == label:\n",
    "                print(f\"  ‚û§ '{key}': {count} samples\")\n",
    "\n",
    "    return examples, label_map, features_data\n",
    "\n",
    "def print_class_distribution(labels, label_map):\n",
    "    \"\"\"Print the distribution of classes in the dataset.\"\"\"\n",
    "    from collections import Counter\n",
    "    reverse_label_map = {v: k for k, v in label_map.items()}\n",
    "    \n",
    "    label_counts = Counter(labels)\n",
    "    print(\"\\nClass Distribution:\")\n",
    "    print(\"-\" * 50)\n",
    "    for label_id, count in sorted(label_counts.items()):\n",
    "        class_name = reverse_label_map.get(label_id, f\"Unknown_{label_id}\")\n",
    "        percentage = (count / len(labels)) * 100\n",
    "        print(f\"{class_name}: {count} ({percentage:.1f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c1bab08a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing 3350 JSON files for variable V241049\n",
      "Progress: 0/3350 files processed\n",
      "Progress: 500/3350 files processed\n",
      "Progress: 1000/3350 files processed\n",
      "Progress: 1500/3350 files processed\n",
      "Progress: 2000/3350 files processed\n",
      "Progress: 2500/3350 files processed\n",
      "Progress: 3000/3350 files processed\n",
      "\n",
      "üìä Summary for variable V241049:\n",
      "  ‚û§ Total JSON files: 3350\n",
      "  ‚û§ Valid examples collected: 7257\n",
      "  ‚û§ Unique labels: 2\n",
      "  ‚û§ Skipped due to missing answers: 0\n",
      "  ‚û§ Skipped due to exclusion list: 87\n",
      "  ‚û§ Skipped (not in include_classes): 872\n",
      "  ‚û§ Included only: ['Donald Trump', 'Kamala Harris']\n",
      "  ‚û§ Final label map: {'Donald Trump': 0, 'Kamala Harris': 1}\n",
      "\n",
      "üîç Class distribution (label IDs): Counter({1: 3987, 0: 3270})\n",
      "  ‚û§ 'Donald Trump': 3270 samples\n",
      "  ‚û§ 'Kamala Harris': 3987 samples\n",
      "\n",
      "Class Distribution:\n",
      "--------------------------------------------------\n",
      "Donald Trump: 3270 (45.1%)\n",
      "Kamala Harris: 3987 (54.9%)\n"
     ]
    }
   ],
   "source": [
    "data_folder = \"/home/tschuetz/shared/datasets/respondents\"\n",
    "variable_code=\"V241049\"\n",
    "include_classes = ['Donald Trump', 'Kamala Harris']\n",
    "# Load data with legitimate features\n",
    "examples, label_map, features_data = load_data(data_folder, variable_code, include_classes=include_classes)\n",
    "\n",
    "# Split texts and labels\n",
    "texts = [ex[0] for ex in examples]\n",
    "labels = [ex[1] for ex in examples]\n",
    "\n",
    "# Print class distribution\n",
    "print_class_distribution(labels, label_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "859f848b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ANESDataset(Dataset):\n",
    "    def __init__(self, texts, labels, tokenizer, max_len=256):\n",
    "        self.texts = list(texts)\n",
    "        self.labels = list(labels)\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_len\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        enc = self.tokenizer(\n",
    "            self.texts[idx],\n",
    "            truncation=True,\n",
    "            padding=\"max_length\",\n",
    "            max_length=self.max_len,\n",
    "            return_tensors=\"pt\",\n",
    "        )\n",
    "\n",
    "        return {\n",
    "            \"input_ids\": enc[\"input_ids\"].squeeze(0).long(),\n",
    "            \"attention_mask\": enc[\"attention_mask\"].squeeze(0).float(),\n",
    "            \"labels\": torch.tensor(self.labels[idx], dtype=torch.long),\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "25ad437d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_split_anes_dataset(texts, labels, tokenizer, max_len=256, seed=42):\n",
    "    \"\"\"\n",
    "    Creates train/val/test splits from raw texts and labels, returning three Dataset objects.\n",
    "    \"\"\"\n",
    "    # Wrap raw data in the ANESDataset\n",
    "    full_dataset = ANESDataset(texts, labels, tokenizer, max_len=max_len)\n",
    "    total = len(full_dataset)\n",
    "    \n",
    "    # Compute split sizes: 80% train, 10% val, 10% test\n",
    "    train_size = int(0.8 * total)\n",
    "    val_size = int(0.1 * total)\n",
    "    test_size = total - train_size - val_size\n",
    "    \n",
    "    # Perform the split with a fixed random seed for reproducibility\n",
    "    train_ds, val_ds, test_ds = random_split(\n",
    "        full_dataset,\n",
    "        [train_size, val_size, test_size],\n",
    "        generator=torch.Generator().manual_seed(seed)\n",
    "    )\n",
    "    \n",
    "    print(f\"Dataset split: {train_size} train, {val_size} val, {test_size} test\")\n",
    "    return train_ds, val_ds, test_ds\n",
    "\n",
    "# Usage example:\n",
    "# texts = [ex[0] for ex in examples]\n",
    "# labels = [ex[1] for ex in examples]\n",
    "# train_ds, val_ds, test_ds = load_split_anes_dataset(texts, labels, tokenizer)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "814f319a",
   "metadata": {},
   "source": [
    "## Train new prediction heads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b9064009",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: './lora_finetuned_model/deepseek-llm-7b-base/'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 14\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [name \u001b[38;5;28;01mfor\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m os\u001b[38;5;241m.\u001b[39mlistdir(path) \u001b[38;5;28;01mif\u001b[39;00m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39misdir(os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(path, name))]\n\u001b[1;32m     13\u001b[0m \u001b[38;5;66;03m# Example usage:\u001b[39;00m\n\u001b[0;32m---> 14\u001b[0m fine_tuned_models \u001b[38;5;241m=\u001b[39m \u001b[43mget_subfolder_names\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m./lora_finetuned_model/deepseek-llm-7b-base/\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Replace '.' with your desired path\u001b[39;00m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;28mprint\u001b[39m(fine_tuned_models)\n",
      "Cell \u001b[0;32mIn[6], line 11\u001b[0m, in \u001b[0;36mget_subfolder_names\u001b[0;34m(path)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_subfolder_names\u001b[39m(path\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m'\u001b[39m):\n\u001b[1;32m      2\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;124;03m    Returns a list of all subfolder names in the specified directory.\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;124;03m        List[str]: A list of subfolder names.\u001b[39;00m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m---> 11\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [name \u001b[38;5;28;01mfor\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m \u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlistdir\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mif\u001b[39;00m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39misdir(os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(path, name))]\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: './lora_finetuned_model/deepseek-llm-7b-base/'"
     ]
    }
   ],
   "source": [
    "def get_subfolder_names(path='.'):\n",
    "    \"\"\"\n",
    "    Returns a list of all subfolder names in the specified directory.\n",
    "\n",
    "    Parameters:\n",
    "        path (str): The directory path to search in. Defaults to the current directory.\n",
    "\n",
    "    Returns:\n",
    "        List[str]: A list of subfolder names.\n",
    "    \"\"\"\n",
    "    return [name for name in os.listdir(path) if os.path.isdir(os.path.join(path, name))]\n",
    "\n",
    "# Example usage:\n",
    "fine_tuned_models = get_subfolder_names('./lora_finetuned_model/deepseek-llm-7b-base/')  # Replace '.' with your desired path\n",
    "print(fine_tuned_models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "961ed8dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def freeze_for_classification_head(model):\n",
    "    \"\"\"\n",
    "    Freeze all parameters except those in the classification head,\n",
    "    which in LlamaForSequenceClassification is under 'score'.\n",
    "    \"\"\"\n",
    "    for name, param in model.named_parameters():\n",
    "        # keep anything with \"score\" in its name\n",
    "        param.requires_grad = \"score\" in name\n",
    "\n",
    "    # Debug: print exactly which parameters remain trainable\n",
    "    unfrozen = [n for n,p in model.named_parameters() if p.requires_grad]\n",
    "    print(f\"‚ñ∂Ô∏è  {len(unfrozen)} trainable params (head only):\\n    \" + \"\\n    \".join(unfrozen))\n",
    "\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    preds = logits.argmax(axis=1)\n",
    "    return {\n",
    "        \"accuracy\": accuracy_score(labels, preds),\n",
    "        \"f1\": f1_score(labels, preds),\n",
    "        \"precision\": precision_score(labels, preds),\n",
    "        \"recall\": recall_score(labels, preds),\n",
    "    }\n",
    "\n",
    "\n",
    "def train_model(model, tokenizer, train_dataset, val_dataset, output_dir):\n",
    "    if not torch.cuda.is_available():\n",
    "        raise RuntimeError(\"No CUDA device found! Training requires a GPU.\")\n",
    "    \n",
    "    # Check where the model currently lives\n",
    "    first_param_device = next(model.parameters()).device\n",
    "    if first_param_device.type != \"cuda\":\n",
    "        print(f\"‚ö†Ô∏è  Model is on {first_param_device}, moving to CUDA\")\n",
    "        model.to(\"cuda\")\n",
    "    else:\n",
    "        print(\"‚úÖ Model is already on GPU\")\n",
    "\n",
    "    args = TrainingArguments(\n",
    "        output_dir=output_dir,\n",
    "        per_device_train_batch_size=8,\n",
    "        num_train_epochs=3,\n",
    "        evaluation_strategy=\"epoch\",\n",
    "        save_strategy=\"epoch\",\n",
    "        logging_steps=10,\n",
    "        report_to=\"none\",\n",
    "        load_best_model_at_end=True,\n",
    "        metric_for_best_model=\"f1\",\n",
    "        fp16=True,\n",
    "        dataloader_num_workers=4,\n",
    "    )\n",
    "\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=args,\n",
    "        train_dataset=train_dataset,\n",
    "        eval_dataset=val_dataset,\n",
    "        tokenizer=tokenizer,\n",
    "        compute_metrics=compute_metrics\n",
    "    )\n",
    "\n",
    "    trainer.train()\n",
    "    return trainer\n",
    "\n",
    "def evaluate_on_test_set(trainer, test_dataset):\n",
    "    results = trainer.evaluate(eval_dataset=test_dataset)\n",
    "    print(\"üìä Test Set Metrics:\")\n",
    "    for k, v in results.items():\n",
    "        if not k.startswith(\"eval_\"):\n",
    "            continue\n",
    "        print(f\"{k.replace('eval_', '')}: {v:.4f}\")\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb3d4a8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# name of used LLM\n",
    "base_model_name = \"deepseek-ai/deepseek-llm-7b-base\"\n",
    "\n",
    "# Load tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(base_model_name)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "# split dataset\n",
    "train_ds, val_ds, test_ds = load_split_anes_dataset(texts, labels, tokenizer)\n",
    "\n",
    "results_list = []\n",
    "\n",
    "for model_ft in fine_tuned_models:\n",
    "    \n",
    "    model = AutoModelForSequenceClassification.from_pretrained(base_model_name, num_labels=2)\n",
    "    model = PeftModel.from_pretrained(model, f\"./lora_finetuned_model/deepseek-llm-7b-base/{model_ft}\")\n",
    "    model.config.pad_token_id = tokenizer.eos_token_id\n",
    "\n",
    "    freeze_for_classification_head(model)\n",
    "\n",
    "    trainer = train_model(model, tokenizer, train_ds, val_ds, f\"./prediction_heads/{model_ft}\")\n",
    "\n",
    "    results = evaluate_on_test_set(trainer, test_ds)\n",
    "\n",
    "    # Clean and append results\n",
    "    cleaned_results = {k.replace(\"eval_\", \"\"): v for k, v in results.items() if k.startswith(\"eval_\")}\n",
    "    cleaned_results[\"model_name\"] = model_ft\n",
    "    results_list.append(cleaned_results)\n",
    "\n",
    "    # clear models from GPU, as the would accumulate and take up too much VRAM\n",
    "    del model\n",
    "    del trainer\n",
    "    torch.cuda.empty_cache()\n",
    "    gc.collect()\n",
    "\n",
    "# Create a DataFrame\n",
    "results_df = pd.DataFrame(results_list)\n",
    "\n",
    "# Optional: save to CSV\n",
    "results_df.to_csv(\"fine_tuned_model_test_metrics.csv\", index=False)\n",
    "\n",
    "# View results\n",
    "print(results_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ef4cfbe2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset split: 5805 train, 725 val, 727 test\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7d2e33d61ad949fda1b902ba0ddd6de8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of LlamaForSequenceClassification were not initialized from the model checkpoint at deepseek-ai/deepseek-llm-7b-base and are newly initialized: ['score.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚ñ∂Ô∏è  1 trainable params (head only):\n",
      "    score.weight\n",
      "‚ö†Ô∏è  Model is on cpu, moving to CUDA\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tschuetz/.conda/envs/tilo_env/lib/python3.10/site-packages/transformers/training_args.py:1575: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ü§ó Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "/tmp/ipykernel_825/2083734758.py:52: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='2178' max='2178' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [2178/2178 08:56, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.573300</td>\n",
       "      <td>0.601620</td>\n",
       "      <td>0.726897</td>\n",
       "      <td>0.803960</td>\n",
       "      <td>0.678930</td>\n",
       "      <td>0.985437</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.559400</td>\n",
       "      <td>0.570672</td>\n",
       "      <td>0.840000</td>\n",
       "      <td>0.869369</td>\n",
       "      <td>0.810924</td>\n",
       "      <td>0.936893</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.512300</td>\n",
       "      <td>0.555795</td>\n",
       "      <td>0.846897</td>\n",
       "      <td>0.875978</td>\n",
       "      <td>0.811594</td>\n",
       "      <td>0.951456</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='91' max='91' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [91/91 00:10]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìä Test Set Metrics:\n",
      "loss: 0.5626\n",
      "accuracy: 0.8294\n",
      "f1: 0.8631\n",
      "precision: 0.7867\n",
      "recall: 0.9560\n",
      "runtime: 11.2238\n",
      "samples_per_second: 64.7730\n",
      "steps_per_second: 8.1080\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "6254"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# name of used LLM\n",
    "base_model_name = \"deepseek-ai/deepseek-llm-7b-base\"\n",
    "\n",
    "# Load tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(base_model_name)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "# split dataset\n",
    "train_ds, val_ds, test_ds = load_split_anes_dataset(texts, labels, tokenizer)\n",
    "    \n",
    "model = AutoModelForSequenceClassification.from_pretrained(base_model_name, num_labels=2)\n",
    "model.config.pad_token_id = tokenizer.eos_token_id\n",
    "\n",
    "freeze_for_classification_head(model)\n",
    "\n",
    "trainer = train_model(model, tokenizer, train_ds, val_ds, f\"./prediction_heads/{'baseline'}\")\n",
    "\n",
    "results = evaluate_on_test_set(trainer, test_ds)\n",
    "\n",
    "# Clean and append results\n",
    "\"\"\" cleaned_results = {k.replace(\"eval_\", \"\"): v for k, v in results.items() if k.startswith(\"eval_\")}\n",
    "cleaned_results[\"model_name\"] = model_ft\n",
    "results_list.append(cleaned_results) \"\"\"\n",
    "\n",
    "# clear models from GPU, as the would accumulate and take up too much VRAM\n",
    "del model\n",
    "del trainer\n",
    "torch.cuda.empty_cache()\n",
    "gc.collect()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tilo_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
